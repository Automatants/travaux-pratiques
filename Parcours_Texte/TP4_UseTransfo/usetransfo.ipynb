{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Utilisation des transformers pré-entrainés et fine-tuning\n",
    "Dans les tps précédents, on a vu comment créer et entraîner un transformer from scratch. Mais souvent, on n'entraine pas un transformer from scratch, mais on utilise un transformer pré-entrainé et on fait du fine-tuning. Le fine-tuning, aussi appelé transfert learning, consiste à prendre un modèle pré-entrainé (dans notre cas GPT-2) et à l'entraîner davantage sur un dataset spécifique. L'idée est que le modèle a déjà appris des caractéristiques générales lors de son entraînement initial, et maintenant, avec le fine-tuning, il peut apprendre des caractéristiques spécifiques à notre problème.\n",
    "\n",
    "### N'oubliez pas de mettre le runtime en GPU si vous êtes sur colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilisation de GPT-2\n",
    "Ici, on va utiliser GPT-2. GPT-2 est pré-entrainé sur un dataset de 8 millions de pages web.\n",
    "\n",
    "Pour ça, on va utiliser la librairie transformers de HuggingFace. Cette librairie permet d'utiliser des modèles pré-entrainés et de les fine-tuner. Elle permet aussi de télécharger des modèles pré-entrainés.\n",
    "On va donc télécharger le modèle GPT-2 et l'utiliser pour générer du texte.\n",
    "\n",
    "Pour installer la librairie, il faut faire `pip install transformers`\n",
    "\n",
    "Pour télécharger le modèle, il faut faire `from transformers import GPT2LMHeadModel, AutoTokenizer`\n",
    "\n",
    "Il existe plusieurs classe de GPT2, par exemple GPT2Model, GPT2LMHeadModel, GPT2ForSequenceClassification, etc.\n",
    "\n",
    "GPT2Model ne contient pas la dernière couche linéaire qui permet de faire la prédiction du mot suivant. GPT2LMHeadModel contient cette dernière couche linéaire. GPT2ForSequenceClassification prend le dernier token et fait une classification dessus.\n",
    "\n",
    "On va utiliser GPT2LMHeadModel, car on veut générer du texte.\n",
    "\n",
    "Pour avoir plus d'informations sur les modèles, il faut aller sur le site de HuggingFace : https://huggingface.co/transformers/model_doc/gpt2.html\n",
    "\n",
    "AutoTokenizer permet de télécharger le tokenizer associé au modèle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Telechargement du modèle et du tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remarque: dans certains tutos, ils utilisent `AutoModelForCausalLM` au lieu de `GPT2LMHeadModel`. C'est la même chose, c'est juste que `AutoModelForCausalLM` est plus général, car il peut être utilisé pour d'autres modèles que GPT-2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exemple d'utilisation du tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"Hello, my dog is cute\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exemple d'utilisation du modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "output = model(input_ids, attention_mask=attention_mask)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comme vous pouvez voir, `output` est un objet de classe `CausalLMOutputWithCrossAttentions`, qui contient plusieurs informations.\n",
    "Si vous lancez la cellule suivante, vous aurez plus d'informations sur cette classe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "help(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut voir que `output` contient `loss`, `logits`, `past_key_values`, `hidden_states`, `attentions`, `cross_attentions`.\n",
    "`loss` est `None` si on ne lui donne pas `labels` en entrée.\n",
    "`logits` est un tenseur de taille `[batch_size, sequence_length, vocab_size]`, qui correspond aux scores du mot suivant pour chaque mot du vocabulaire. Si on applique un softmax à `logits`, on obtient la probabilité du mot suivant pour chaque mot du vocabulaire.\n",
    "\n",
    "Si on prend l'argmax du dernier token de `logits`, on obtient le mot suivant le plus probable.\n",
    "\n",
    "Vous pouvez regarder par vous-même les autres attributs de `output`.\n",
    "\n",
    "Exemple :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(output.loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vous remarquez que `loss` est `None`, car on n'a pas donné `labels` en entrée. Pour donner `labels` en entrée, il faut faire ajouter dans le dictionnaire `tokens` la clé \"labels\" avec comme valeur `input_ids`.\n",
    "Par exemple:\n",
    "```python\n",
    "tokens[\"labels\"] = input_ids\n",
    "```\n",
    "\n",
    "En effet, on n'a pas besoin de \"shift\" les labels, car le shifting est déjà fait dans le modèle GPT-2.\n",
    "Pour rappel: le shifting consiste à décaler les labels d'un token vers la gauche. Par exemple, si on a le texte \"Hello, my dog is cute\", le shifting consiste à avoir comme labels \"my dog is cute\" et comme input \"Hello, my dog is\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(type(output.logits))\n",
    "print(output.logits.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut voir que `logits` est un tenseur de taille `[1, 6, 50257]`. 1 correspond à la taille du batch, 6 correspond à la taille de la séquence, et 50257 correspond à la taille du vocabulaire."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice:\n",
    "Sachant que `tokenizer.bos_token` correspond au token de début de séquence (**B**eginning **O**f **S**entence), `tokenizer.eos_token` correspond au token de fin de séquence (**E**nd **O**f **S**entence), et que `tokenizer.decode` permet de transformer des ids en texte, esayez de générer du texte avec le modèle GPT-2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.decode([15496, 11, 616, 3290, 318, 13779]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remarquez que pour GPT2, bos_token = eos_token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Votre code ici"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning de GPT-2\n",
    "On pourrait écrire nous-même le code pour faire du fine-tuning (c'est juste un entraînement classique, c'est juste que les poids de départ sont ceux du modèle pré-entrainé), , mais heureusement, la librairie transformers le fait pour nous. Il faut utiliser `Trainer` et `TrainingArguments` de la librairie transformers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Utilisation de `distilgpt2` pour faire du fine-tuning\n",
    "En fait, GPT2 est beaucoup trop gros pour être utilisé sur colab. On va donc utiliser `distilgpt2`, qui est une version plus petite de GPT2. `distilgpt2` a été pré-entraîné sur le même dataset que GPT2, mais il est 2 fois plus petit que GPT2.\n",
    "\n",
    "Pour utiliser \"distilgpt2\", il suffit de remplacer \"gpt2\" par \"distilgpt2\" quand vous faites `from_pretrained(\"gpt2\")`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utilisation de distilgpt2\n",
    "# votre code ici\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation du dataset\n",
    "On va télécharger le même dataset qu'on a utilisé pour entraîner notre modèle from scratch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/colab_train.txt\"\n",
    "r = requests.get(url)\n",
    "dataset = r.text\n",
    "\n",
    "# save file\n",
    "with open(\"colab_train.txt\", \"w\") as f:\n",
    "\tf.write(dataset)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/colab_train_prompts.txt\"\n",
    "r = requests.get(url)\n",
    "dataset_prompt = r.text\n",
    "\n",
    "# save file\n",
    "with open(\"colab_train_prompts.txt\", \"w\") as f:\n",
    "\tf.write(dataset_prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans le fichier `colab_train_prompts.txt`, chaque ligne correspond à un prompt, et dans le fichier `colab_train.txt`, chaque ligne correspond à l'histoire associée au prompt de `colab_train_prompts.txt`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Création du dataset\n",
    "Maintenant, il faut créer le dataset pour le fine-tuning. On va creer un custom dataset avec la classe `Dataset` de pytorch. On va créer un dataset qui prend en entrée un prompt et qui renvoie l'histoire associée au prompt.\n",
    "\n",
    "Pour créer un dataset avec la classe `Dataset` de pytorch, il faut créer une classe qui hérite de `Dataset` et qui a comme attributs `__len__` et `__getitem__`.\n",
    "`__len__` renvoie la taille du dataset, et `__getitem__` renvoie l'élément à l'index donné en entrée.\n",
    "\n",
    "Dans notre cas, `__getitem__` doit renvoyer un dictionnaire avec les clés \"input_ids\". Comme vous avez vu dans la partie précédente, \"input_ids\" et il correspond aux ids de \"{prompt} {histoire}\".\n",
    "Pas besoin de créer les \"labels\".\n",
    "\n",
    "Il faut donc combiner les ids du prompt et de l'histoire pour avoir les ids de \"{prompt} {histoire}\".\n",
    "\n",
    "Si la longueur de \"{prompt} {histoire}\" est supérieure à 1024, il faut couper \"{prompt} {histoire}\" en plusieurs morceaux de taille 1024, car GPT-2 ne peut pas prendre en entrée des séquences de taille supérieure à 1024."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\t# votre code ici\n",
    "\t\t# lire le fichier colab_train.txt et colab_train_prompts.txt et convertir en liste de str\n",
    "\t\t# vous devriez avoir 2 listes de même taille\n",
    "\t\tpass\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t# votre code ici\n",
    "\t\t# retourner len(liste de str)\n",
    "\t\tpass\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# votre code ici\n",
    "\t\t# tokenizer le prompt et l'histoire à l'index idx\n",
    "\t\t# concatener les input_ids du prompt et de l'histoire\n",
    "\t\t# renvoyer un dictionnaire avec les clés \"input_ids\"\n",
    "\t\tpass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilisation de `Trainer` et `TrainingArguments`\n",
    "\n",
    "Maintenant, on va utiliser `Trainer` et `TrainingArguments` pour faire du fine-tuning.\n",
    "`TrainingArguments` permet de définir les arguments d'entraînement, comme le nombre d'epochs, la taille du batch, etc.\n",
    "`Trainer` permet de faire l'entraînement, et il prend en entrée un `TrainingArguments`, un `model`, un `data_collator`, et un `train_dataset`.\n",
    "\n",
    "Un `data_collator` est un objet `DataCollator` de la librairie transformers. Il permet de combiner les données en batch (ce qui est pratique car ça nous évite de pad nous-même les données). Il existe plusieurs `DataCollator`, et on va utiliser `DataCollatorForLanguageModeling`, qui permet de combiner les données en batch pour faire du language modeling.\n",
    "\n",
    "Pour créer un `DataCollatorForLanguageModeling`, on lance la cellule suivante:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour tester si le `data_collator` et le `dataset` fonctionnent bien ensemble, on peut faire:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "out = data_collator([dataset[i]] for i in range(5))\n",
    "\n",
    "for key in out:\n",
    "\tprint(f\"{key} shape: {out[key].shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vous devriez avoir comme sortie:\n",
    "```\n",
    "input_ids shape: torch.Size([5, 1024])\n",
    "attention_mask shape: torch.Size([5, 1024])\n",
    "labels shape: torch.Size([5, 1024])\n",
    "```\n",
    "\n",
    "On obtient bien des batchs de taille 5, avec des input_ids, des attention_mask et des labels de taille 1024."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du `TrainingArguments` et du `Trainer`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./gpt2\",\n",
    "\tnum_train_epochs=1,\n",
    "\tper_device_train_batch_size=4,\n",
    "\tsave_steps=10,\n",
    "\tsave_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\tdata_collator=data_collator,\n",
    "\ttrain_dataset=dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Entraînement du modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercice: Génération de texte\n",
    "Maintenant que le modèle est entraîné, on va générer du texte avec le modèle fine-tuné.\n",
    "Vous pouvez utiliser `model` comme on l'a fait précédemment.\n",
    "\n",
    "Essayez de générer l'histoire du premier prompt du fichier `colab_train_prompts.txt`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# votre code ici"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilisation de BERT\n",
    "BERT est un autre transformer pour faire du NLP. Il est plus utilisé pour faire de la classification de texte que pour faire de la génération de texte.\n",
    "\n",
    "BERT veut dire Bidirectional Encoder Representations from Transformers. Il est bidirectionnel car lors du pretraining, on ne met pas de causal mask, donc le modèle peut voir les mots futurs, contrairement à un modèle comme GPT-2 qui ne peut que voir les mots précédents (et donc unidirectionel).\n",
    "\n",
    "Lors du pretraining, on utilise 2 tâches: Masked Language Modeling (MLM) et Next Sentence Prediction (NSP).\n",
    "Le MLM consiste à masquer des mots dans une phrase et à faire prédire les mots masqués (en utilisant les mots précédents et les mots suivants du mot masqué). L'intuition est que le modèle doit comprendre le contexte pour pouvoir prédire le mot masqué.\n",
    "\n",
    "Le NSP consiste à donner 2 phrases en entrée et à faire prédire si la deuxième phrase suit la première phrase.\n",
    "Idem, l'intuition est que le modèle doit comprendre le contexte pour pouvoir prédire si la deuxième phrase suit la première phrase.\n",
    "\n",
    "Pour plus d'informations sur BERT, vous pouvez lire le papier original: https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "Dans ce TP, on va utilisé une versio fine-tunée de BERT pour faire du Question Answering (QA). Le QA consiste à donner une question et une réponse en entrée et à faire prédire le \"score\" de cohérence entre la question et la réponse.\n",
    "\n",
    "Donc par exemple si on a la question \"What color is the sky?\" et deux réponses \"The sky is blue\" et \"I love orange\", le modèle doit prédire un score plus élevé pour \"The sky is blue\" que pour \"I love orange\".\n",
    "\n",
    "Pour ça, on va utiliser `multi-qa-mpnet-base-dot-v1`, qui est un modèle fine-tuné de BERT pour faire du QA.\n",
    "On va passer en entrée la question dans le modèle, pour avoir un vecteur de taille 768, et représente la question.\n",
    "On va faire la même chose pour les réponses.\n",
    "\n",
    "On a alors un vecteur $Q$ qui représente la question et deux vecteurs $R_1$ et $R_2$ qui représentent les réponses.\n",
    "Ensuite, on va calculer le produit scalaire entre $Q$ et $R_1$ et entre $Q$ et $R_2$ pour avoir 2 scores $S_1$ et $S_2$.\n",
    "\n",
    "Puis on compare $S_1$ et $S_2$ pour savoir quelle réponse est la plus cohérente avec la question.\n",
    "\n",
    "\n",
    "Vous pouvez avoir plus d'informations sur les senteces transformers ici: https://www.sbert.net/docs/pretrained_models.html\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilisation de `multi-qa-mpnet-base-dot-v1`\n",
    "Pour utiliser `multi-qa-mpnet-base-dot-v1`, il faut installer la librairie `sentence-transformers` avec `pip install sentence-transformers`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour encoder une phrase, il suffit de faire:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.encode(\"How are you?\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Calcul du vecteur embedding de la question et des réponses et calcul du score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "question_embedding = ...\n",
    "answer_embedding_1 = ...\n",
    "answer_embedding_2 = ...\n",
    "\n",
    "score_1 = ...\n",
    "score_2 = ...\n",
    "\n",
    "print(f\"score 1: {score_1}\")\n",
    "print(f\"score 2: {score_2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovvEtcCS1rH6"
   },
   "source": [
    "# Projet de permanence : Découverte des outils de classification et de clustering\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Un peu de curiosité et de patience feront l'affaire ! ;)\n",
    "\n",
    "Plus sérieusement, il vous faut soit [Google colab'](https://colab.research.google.com/), soit un environnement Python avec `tensorflow`, `numpy`, `keras`et `scikit-learn`.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "L'objectif de ce sujet est de vous faire découvrir les librairies **Keras** et **scikit-learn** en vous permettant d'avoir vos premiers résultats de **Machine Learning** avec le dataset **CIFAR10**.\n",
    "\n",
    "Nous allons aborder les notions suivantes : les couches denses, les CNNs, les auto-encodeurs, le clustering, ...\n",
    "\n",
    "Le plan est le suivant :\n",
    "\n",
    "1. **La classification supervisée :** Prédiction du label d'une image grâce à un réseau de neurones\n",
    "2. **Les autoencodeurs :** Concept et 3 applications qui vont changer votre vie !\n",
    "3. **La classification non-supervisée :** Le secret derrière Argos.\n",
    "\n",
    "Il y a même une petite compétition à la fin !\n",
    "\n",
    "Vous remarquerez en faisant le TP que les instructions sont de moins en moins détaillées au fur et à mesure, cela est normal. Une fois que quelque chose a été vu, il n'est plus autant détaillé la fois d'après. Donc n'hésitez pas à revenir en arrière pour voir comment vous aviez fait au début.\n",
    "\n",
    "Ci-dessous une petite FAQ pour vous présenter certains points. Si vous avez des questions ou des remarques, n'hésitez pas à les poser aux 2As présents en perm' ou sur le groupe Automatants.\n",
    "\n",
    "**Good luck, have fun !**\n",
    "\n",
    "**Avertissement :** Dans ce TP, et plus particulièrement vers la fin, vous serez amenés à manipuler des méthodes et concepts que vous n'avez peut-être (sûrement) jamais vus et que nous n'avons pas encore expliqués dans les formations. Ce TP va vous permettre de les utiliser mais il n'y aura que très peu d'explications théoriques. On vous invite à lire les références que l'on vous donne pour comprendre le fonctionnement général des algorithmes. La plupart de ces méthodes vous seront présentées d'un point de vue plus théorique plus tard.\n",
    "\n",
    "## FAQ\n",
    "\n",
    "### Comment ça marche un fichier Jupyter ?\n",
    "\n",
    "Les fichiers Jupyter vous permettent d'exécuter du Python depuis votre navigateur, tout en pouvant alterner morceaux de code et morceaux de textes. Afin d'éxécuter du code, il faut appuyer sur `Crtl` + `R` (Reste sur le bloc courant) ou `Shift` + `R` (Exécute le bloc courant et passe au bloc suivant).\n",
    "\n",
    "Tous les `print` ou `plot` que vous ferez s'afficheront directement en dessus du bloc de code.\n",
    "\n",
    "### Qu'est-ce qu'un dataset ?\n",
    "\n",
    "Un **dataset** est un jeu de données, qui généralement associe à des données d'entrée une ou plusieurs données de sortie.\n",
    "\n",
    "Par exemple, le dataset MNIST associe à des images en nuance de gris le chiffre qu'elle représente.\n",
    "\n",
    "Ici, nous allons manipuler le dataset CIFAR10 qui à des images en couleur associe le nom de de ce qu'elle représente (Plus de détails dans la suite).\n",
    "\n",
    "La construction d'un dataset bien labelisé et conséquent est une des difficultés majeures en Machine Learning. Heureusement pour nous, de nombreux datasets existent et sont mis à disposition de tous !\n",
    "\n",
    "### Mais c'est quoi Keras?\n",
    "\n",
    "**Keras** est une libraire qui permet de rapidement créer des réseaux de neurones et de les entrainer. De nombreuses couches sont déjà codées, telles que les couches denses (du multi-perceptron), des CNNs, des couches récurrentes, etc., ce qui permet de n'avoir qu'à les assembler. Keras inclut également plusieurs datasets, ce qui est très pratique pour tester des algorithmes.\n",
    "\n",
    "En réalité, Keras est une sur-couche de TensorFlow.\n",
    "\n",
    "### C'est qui TensorFlow ?\n",
    "\n",
    "**Tensorflow** est une librairie développée par Google pour créer et manipuler des réseaux de neurones sans avoir à se lancer dans un code fastidieux. Il existe d'autres libraires, comme PyTorch (Facebook) ou Caffe (Berkley). PyTorch et Tensorflow sont les plus répandus dans l'industrie et la recherche.\n",
    "\n",
    "\n",
    "Depuis peu, Tensorflow intègre une sur-couche (appelée Keras) qui facilite encore plus la construction des réseaux. C'est cette sur-couche que nous utiliserons pour ce TP.\n",
    "\n",
    "\n",
    "Sachez qu'il existe de nombreuses façons d'utiliser Tensorflow/Keras, nous vous présentons la plus simple à utiliser pour débuter, mais sachez que chaque étape de l'apprentissage peut être détaillée et personnalisée sans limite, et que des architectures de réseaux complexes sont possibles.\n",
    "\n",
    "### Et scikit-learn ?\n",
    "\n",
    "De son côté **scikit-learn** propose beaucoup d'outils pour faire de l'analyse prédictive de données (Classification, régression, clustering, réduction de dimension, etc. *Beaucoup de termes barbares que vous allez découvrir dans la suite*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DoAzN2f1rH9"
   },
   "source": [
    "# Import des librairies et des données\n",
    "\n",
    "## Les librairies\n",
    "\n",
    "Commençons par importer les librairies dont nous aurons besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1G1uGxc1rIA"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0raS2D1U1rIR"
   },
   "source": [
    "## Le dataset\n",
    "\n",
    "Importons maintenant le dataset. Nous allons utiliser CIFAR10, dont voilà une description :\n",
    "\n",
    "> L'ensemble de données CIFAR10 contient 60 000 images couleur de 32x32 dans 10 classes, avec 6 000 images dans chaque classe. L'ensemble de données est divisé en 50 000 images d'entraînement et 10 000 images de test. Les classes sont mutuellement exclusives et il n'y a pas de chevauchement entre elles.\n",
    "\n",
    "Pour rappel, il est important de séparer les données en données d'entrainement et données de test. Cela permet de constater si notre modèle a effectué un sur-apprentissage (overfitting) et est incapable de généraliser ce qu'il a appris.\n",
    "\n",
    "**Keras** inclut de nombreux datasets dans son modèle `datasets` ([liste des datasets](https://keras.io/api/datasets/)) que l'on peut importer avec la fonction `load_data`.\n",
    "\n",
    "Les valeurs des pixels varient entre 0 et 255. Nous allons les normaliser pour qu'elles soient entre 0 et 1, ce qui est plus adapté pour les réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9RUtkkp1rIg"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaNhvUQt1rIn"
   },
   "source": [
    "Visualisons un peu toutes ces images !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jSND_dO1rIp"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9ZsfFfD1rIx"
   },
   "source": [
    "Parfait, nous pouvons maintenant commencer les choses sérieuses !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v36aaMiL1rI0"
   },
   "source": [
    "# Classification supervisée\n",
    "\n",
    "## Qu'est-ce que la classification supervisée ?\n",
    "\n",
    "L'idée est de trouver une fonction de prédiction à partir des données annotées. Ici, cela revient à trouver la fonction qui à chaque image associe son label.\n",
    "\n",
    "Cela est différent de l'apprentissage non-supervisé que l'on fera dans la suite.\n",
    "\n",
    "## Comment allons-nous faire ?\n",
    "\n",
    "Les images font 32 pixels par 32 avec 3 valeurs pour chaque couleur (**R**ouge, **V**ert, **B**leu ou **RVB** en français et **RGB** en anglais). Nous avons donc 32 x 32 x 3 = 3072 paramètres ...\n",
    "\n",
    "Nous allons donc utiliser des réseaux de neurones pour approximer notre fonction de prédiction !\n",
    "\n",
    "## Avec un réseau dense\n",
    "\n",
    "### Rappel théorique\n",
    "\n",
    "Si vous assistez aux premières formations, vous savez ce qu'est un réseau dense et si vous avez assisté au TP Multiperceptron, vous avez même codé votre propre réseau dense !\n",
    "\n",
    "Pour ceux qui ne s'en souvienne plus, un réseau de neurones dense est composée de plusieurs couches, elles-mêmes composées de plusieurs noeuds. Chaque noeud est relié à tous les noeuds de la couche précédente, dont il pondère la somme des valeurs par des poids, et décale la valeur obtenue par un biais. Finalement, pour éviter que l'ensemble de toutes les couches se résument à une seule couche, on casse la linéarité en associant à chaque couche une fonction d'activation (sigmoïde, tanh, relu, etc.).\n",
    "\n",
    "C'est une explication très très succinte. Pour mieux comprendre ce que c'est, vous pouvez revoir la formation sur les premières formations, ou il y a ces excellentes vidéos de [Science4All, Les réseaux de neurones | Intelligence artificielle 41](https://www.youtube.com/watch?v=8qL2lSQd9L8) (FR) et de [3Blue1Brown, But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk) (ANG, mais très visuel).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/220px-Neural_network.svg.png)\n",
    "\n",
    "La première couche est appelée `Input layer`, car c'est là que sont insérées les données d'entrée. La dernière couche est appelée `Output layer` et correspond aux valeurs de sortie. Les couches intermédiaires sont appelées `Hidden layer`.\n",
    "\n",
    "### En pratique\n",
    "\n",
    "Pour créer un modèle avec **Keras**, on peut utiliser `keras.models.Sequential()`.\n",
    "\n",
    "**Instruction :** Créer un modèle que vous assignerez à `model_dense1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIzviHUn1rI1"
   },
   "outputs": [],
   "source": [
    "model_dense1 = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REuZJ2tF1rI-"
   },
   "source": [
    "Il est maintenant temps d'ajouter les premières couches à ce modèle. On pourra utiliser :\n",
    "- `model_dense1.add(layer)` qui ajoute layer à la suite des couches du modèle.\n",
    "\n",
    "\n",
    "- `keras.layers.Dense(n_noeuds, activation='...')` qui renvoie une couche dense avec `n_noeuds` noeuds et comme fonction d'activation celle que vous avez précisez (les options possibles sont notamment `sigmoid`, `tanh`, `relu`, `softmax`)\n",
    "\n",
    "  Plus d'infos dans la doc de Keras : [Dense Layer](https://keras.io/api/layers/core_layers/dense/).\n",
    "\n",
    "\n",
    "- `keras.layers.Flatten(input_shape=shape)` qui renvoie une couche qui *aplatit* les données d'entrée en une liste à la sortie.\n",
    "\n",
    "  `input_shape` est un tuple qui permet de préciser le format des données d'entrée (Exemple (32, 32, 3) pour les images de CIFAR10).\n",
    "\n",
    "Maintenant, réfléchissons un peu. Nos données d'entrée sont de la forme (32, 32, 3). Or, une couche dense ne peut prendre qu'une liste en entrée ... Il va donc falloir utiliser utiliser une première couche `Flatten` pour avoir le bon format.\n",
    "\n",
    "**Instruction :** Rajoutez une couche `Flatten` au modèle.\n",
    "\n",
    "*Note :* On pourrait également modifier les données d'entrée, mais vu que dans la suite, on va remanier les images, ça ne serait pas pratique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDbNW6xZ1rJA"
   },
   "outputs": [],
   "source": [
    "model_dense1.add(keras.layers.Flatten(input_shape=(32, 32, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-loxVDsI1rJF"
   },
   "source": [
    "À la sortie de cette couche, nous avons donc une liste de 32 x 32 x 3 valeurs. On peut donc enfin mettre une première couche cachée !\n",
    "\n",
    "**Instruction :** Rajoutez une couche `Dense` avec 512 noeuds et la fonction d'activation `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfYQExy11rJH"
   },
   "outputs": [],
   "source": [
    "model_dense1.add(keras.layers.Dense(512, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9IYVw0O1rJO"
   },
   "source": [
    "Ajoutons maintenant la couche de sortie, on a 10 classes possibles. Notre dernière couche aura donc 10 noeuds, et on aimerait que notre réseau prédise la probabilité de chaque classe, on utilisera donc la fonction d'activation `softmax`.\n",
    "\n",
    "**Instruction :** Rajoutez une couche `Dense` avec 10 noeuds et la fonction d'activation `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "953uQvae1rJP"
   },
   "outputs": [],
   "source": [
    "model_dense1.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULwm91yMyqUL"
   },
   "source": [
    "#### Visualisation du modèle, et compilation\n",
    "\n",
    "Vous avez créé votre premier modèle, félicitations ! Voyons voir un peu ce qu'il contient.\n",
    "\n",
    "La fonction `summary()` affiche un résumé des couches de votre modèle, ainsi que le nombres de paramètres entrainables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekjBBZD8yqUN"
   },
   "outputs": [],
   "source": [
    "model_dense1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-optDYQNyqUV"
   },
   "source": [
    "Avant d'entrainer notre modèle, il faut qu'on le compile pour l'entrainement. Cela permet notamment de préciser certains paramètres.\n",
    "\n",
    "On utilisera la fonction `compile(optimizer=\"adam\", loss=..., metrics=...)` :\n",
    "- `optimizer`\n",
    "  \n",
    "  Méthode utilisée lors de la backpropagation. Ici, on utilisera Adam, qui est beaucoup une méthode d'optimisation très efficace, plus efficace que la descente de gradient normale que vous avez vu en formation.\n",
    "\n",
    "  Plus d'infos sur les optimisateurs : [Various Optimization Algorithms For Training Neural Network](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6).\n",
    "  \n",
    "  \n",
    "- `loss`\n",
    "\n",
    "  Précise comment on calcule l'erreur. On utilisera `tf.keras.losses.SparseCategoricalCrossentropy()` ici.\n",
    "  \n",
    "- `metrics`\n",
    "\n",
    "  Liste des métriques qu'on aimerait suivre. Celle la précision nous intéresse ici, on prendra donc `['accuracy']`.\n",
    "  \n",
    "**Instruction :** Compiler votre modèle avec `model_dense1.compile(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3eTI3TYyqUW"
   },
   "outputs": [],
   "source": [
    "model_dense1.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDVi8iQoyqUd"
   },
   "source": [
    "#### Entrainement\n",
    "\n",
    "Notre modèle est fin prêt pour être entrainé !\n",
    "\n",
    "On utilisera :\n",
    "\n",
    "`history = model_dense1.fit(x, y, epochs=..., validation_data=(x_test, y_test))`\n",
    "\n",
    "- `x`\n",
    "  \n",
    "  Les données d'entrée, ici `train_images`.\n",
    "  \n",
    "  \n",
    "- `y`\n",
    "  \n",
    "  Les données de sortie, ici `train_labels`.\n",
    "  \n",
    "  \n",
    "- `epochs`\n",
    "  \n",
    "  Nombre d'époques à faire pour entrainer notre modèle. Une époque correspond à un passage sur l'ensemble des données `x` et `y`.\n",
    "  \n",
    "  On pourra prendra 5 ici.\n",
    "  \n",
    "  \n",
    "- `validation_data=(x_test, y_test)`\n",
    "\n",
    "  Couple de données entrée / sortie qui ne sont pas utilisées pour entrainer le modèle. Une validation est faite à chaque fin d'époque, et cela permet de voir si notre réseau réussit à généraliser à des nouvelles données ou non.\n",
    "  \n",
    "  Nos données de validation sont `test_images` et `test_labels` ici.\n",
    "  \n",
    "La fonction `fit()` renvoie l'historique des métriques au cours de l'entrainement.\n",
    "\n",
    "**Instruction :** Entrainer votre modèle avec `model_dense1.fit(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xqhs-ZYyqUe"
   },
   "outputs": [],
   "source": [
    "history = model_dense1.fit(train_images, train_labels,\n",
    "                           epochs=5,\n",
    "                           validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk6aAKYHyqUo"
   },
   "source": [
    "Durant l'entrainement, vous voyez pour chaque époque : l'avancement, le temps restant estimé, la loss et la précision. Une fois que l'époque est finie, vous voyez également val_loss et val_accuracy, qui sont la loss et la précision sur les données de test.\n",
    "\n",
    "Affichons les données de l'historique, et voyons comment notre modèle s'en sort !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdk7Iqyd1rJe"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdh9q2w_yqUw"
   },
   "source": [
    "On constate qu'on a une précision d'environ 45%, ce qui est mieux que le hasard, mais n'est pas exceptionnel non plus.\n",
    "\n",
    "Cependant, on peut effectuer une analyse plus détaillé de notre modèle à l'aide de la matrice de confusion.\n",
    "\n",
    "#### La matrice de confusion, késako ?\n",
    "\n",
    "La précision nous indique seulement le ratio $p = \\frac{\\text{prédiction correcte}}{\\text{totale prédiction}}$. On peut utiliser le même ratio, mais pour une classe en particulier.\n",
    "\n",
    "Par exemple, prenons uniquement les images de chat. On effectue les prédictions. Si notre modèle était parfait, il nous dirait qu'on a 1000 chats. Or, il nous dira plutôt qu'on 450 chats, 300 chiens, 150 bateaux et 100 voitures. La précision est alors $p = \\frac{450}{1000} = 0.45$, mais on constate que notre modèle a énormément de mal à différencier les chiens et les chats, il les confond ... On peut répéter ça pour chaque classe, et la matrice qui stocke en ligne le nombre de fois qu'une classe a été prédite pour une même classe initiale.\n",
    "\n",
    "Vous voyez que c'est assez simple à faire en place, il suffit de mieux regarder nos prédictions ... Mais nous sommes des flemmards, nous allons utiliser la fonction `confusion_matrix` de **scikit-learn**, qui nous renvoie une matrice de confusion.\n",
    "\n",
    "---\n",
    "\n",
    "`confusion_matrix(y_true, y_pred, normalize=None)`\n",
    "\n",
    "- `y_true` : le vecteur des vrais labels.\n",
    "- `y_pred` : le vecteur des labels prédits.\n",
    "- Si `normalize=None`, alors la matrice contiendra le nombre d'occurences.\n",
    "- Si `normalize='true'`, alors la matrice contiendra les mêmes informations, mais normalisées entre 0 et 1.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions :**\n",
    "1. Faites les prédictions avec notre modèle à l'aide de `model_dense1(data)` et pour `data`, on prendra les images de test.\n",
    "\n",
    "   Cela vous renvoie normalement une liste de vecteurs de 10 colonnes, dont la somme des valeurs vaut 1 (grâce à la fonction d'activation softmax) et où la plus grande valeur correspond à la classe la plus probable d'après notre modèle.\n",
    "   \n",
    "2. Transformez cette ligne de vecteurs de 10 colonnes en une liste des indices des classes prédites.\n",
    "\n",
    "   Par exemple, le vecteur prédit [0.05, 0.05, 0.1, .8, 0, 0, 0, 0, 0, 0] deviendrait 3, car c'est l'indice de la valeur maximale, donc de la classe la plus probable.\n",
    "   \n",
    "3. Utilisez la fonction `confusion_matrix` pour récupérer la matrice de confusion que vous stockera dans la variable `cm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8Vbj1hMyqUw"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred_labels = np.argmax(model_dense1(test_images), axis=1)\n",
    "cm = confusion_matrix(test_labels, pred_labels, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhXILb7dyqU4"
   },
   "source": [
    "Il ne reste plus qu'à l'afficher pour voir ce que ça donne ! On définit une fonction `show_confusion_matrix` qui nous fait un affichage joli, elle prend en paramètre la matrice de confusion, et le nom des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCDsROJ8yqU6"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(matrix, labels):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(matrix)\n",
    "    \n",
    "    N = len(labels)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(N))\n",
    "    ax.set_yticks(np.arange(N))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Matrice de confusion\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "show_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiM_uAjmyqVF"
   },
   "source": [
    "Comment analyser ces résultats ? Ils se lisent en ligne, et nous indique pour chaque classe, quelle classe est prédite et en quelle proportion. Ainsi, les valeurs diagonales sont les prédictions correctes.\n",
    "\n",
    "Un modèle parfait aurait une matrice de confusion diagonale, car il ne se tromperait jamais.\n",
    "\n",
    "### Allez plus loin\n",
    "\n",
    "Vous pouvez vous amuser à recréer un autre modèle en rajoutant plus de couches avec des paramètres différents, et voir si vous arrivez à atteindre une meilleure précision ! ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBZXPfHZyqVH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8aozBTX1rJk"
   },
   "source": [
    "## Avec des CNNs\n",
    "\n",
    "Jusqu'ici, nous avons utilisé des couches denses pour notre réseau de neurones, mais cela n'est pas le plus adapté pour traiter des images. En effet, cela n'est pas insensible aux translations, etc. C'est pourquoi nous allons utiliser des CNN (Convolutional Neural Network) dans la suite.\n",
    "\n",
    "### Un peu de théorie\n",
    "\n",
    "Les CNNs sont spécifiquement faits pour travailler sur des images !\n",
    "\n",
    "Chaque image est un tableau de taille *largeur x longueur x canaux* (les canaux peuvent être au nombre de 3 pour une image couleur ou 1 pour une image en nuances de gris. On applique alors un filtre, qui est un tableau de largeur et longueur inférieures, mais de nombres de canaux identiques ! Le filtre sert de coefficients pour sommer la valeur des pixels en dessous, et on décale le filtre jusqu'à avoir couvert toute l'image.\n",
    "\n",
    "Ci-dessus un exemple.\n",
    "\n",
    "![](https://miro.medium.com/max/413/1*4yv0yIH0nVhSOv3AkLUIiw.png)\n",
    "![](https://miro.medium.com/max/268/1*MrGSULUtkXc0Ou07QouV8A.gif)\n",
    "\n",
    "Voilà, quelques filtres communs, on voit que cela peut servir pour la détection de bord par exemple !\n",
    "\n",
    "![](https://miro.medium.com/max/349/1*uJpkfkm2Lr72mJtRaqoKZg.png)\n",
    "\n",
    "Il y a d'autres paramètres qu'on ne détaillera pas ici. Au sein d'une couche CNN, il peut y avoir plusieurs filtres. La sortie est alors une image de même largeur et longueur, mais le nombre de canaux est alors égale au nombre de filtres. L'entrainement d'un CNN revient à déterminer les coefficients de ces filtres.\n",
    "\n",
    "Cependant, vous remarquerez qu'ici, on ne diminue pas la taille d'une image, mais qu'on joue seulement sur la profondeur de celle-ci. Afin, de réduire la dimension de celle-ci, nous allons utiliser du `Pooling`, notamment, du `MaxPooling` qui revient à considérer la valeur maximale en déplaçant un filtre à chaque fois (Voir l'illustration ci-dessous).\n",
    "\n",
    "![](https://miro.medium.com/max/602/1*SmiydxM5lbTjoKWYPiuzWQ.png)\n",
    "\n",
    "La `MaxPooling` réduit donc la surface de l'image, tout en gardant la même profondeur. Par exemple, avoir un carré de *2x2* pour le `MaxPooling` divisera la largeur et la longeur de l'image sortante par 2.\n",
    "\n",
    "Le fait de chainer des CNNs permet au modèle d'apprendre des motifs complexes. Les premiers CNNs détecteront les bords, etc. Tandis que les suivants pourront agréger tout ça pour reconnaitre des formes plus évoluées. Après les CNNs, nous pouvons mettre un réseau dense, afin de faire la classification à partir des *features* extraites des CNNs.\n",
    "\n",
    "### En pratique\n",
    "\n",
    "Avec **Keras**, on peut définir un CNN avec les couches suivantes :\n",
    "\n",
    "- `keras.layers.Conv2D(n_filtres, longueur_filtre, activation='relu')`\n",
    "\n",
    "  - `n_filtres` : Le nombre de filtres du CNN\n",
    "  \n",
    "  - `longueur_filtre` : Définit la largeur du filtre qui sera un carré.\n",
    "  \n",
    "  - `activation` : Fonction d'activation, on prendra `'relu'`\n",
    "  \n",
    "  \n",
    "- `keras.layers.MaxPooling(largeur)`\n",
    "\n",
    "  - `largeur` : Largeur du carré du Pooling\n",
    "  \n",
    "Dans la partie CNN du modèle, on alternera les couches `Conv2D` et `MaxPooling2D`.\n",
    "  \n",
    "Dans la définition de notre modèle, puisque les CNNs prennent en entrée une image, nous n'avons plus besoin de la couche `Flatten` au début.\n",
    "\n",
    "#### Définition du modèle\n",
    "\n",
    "**Instruction :**\n",
    "1. Créer un modèle que vous assignerez à `model_cnn1`.\n",
    "2. Rajoutez une couche `Conv2D` avec 64 filtres, une largeur de 3, la fonction d'activation relu et input_shape=(32, 32, 3) (Il faut préciser pour la première couche le format des données d'entrée).\n",
    "3. Rajoutez une couche `MaxPooling2D` de largeur 2.\n",
    "2. Rajoutez une couche `Conv2D` avec 64 filtres, une largeur de 3 et la fonction d'activation relu.\n",
    "3. Rajoutez une couche `MaxPooling2D` de largeur 2.\n",
    "5. Rajoutez une couche `Conv2D` avec 64 filtres, une largeur de 3 et la fonction d'activation relu.\n",
    "\n",
    "Cela correspondra à notre partie CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POiQnqBxyqVQ"
   },
   "outputs": [],
   "source": [
    "model_cnn1 = models.Sequential()\n",
    "model_cnn1.add(layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)))\n",
    "model_cnn1.add(layers.MaxPooling2D(2))\n",
    "model_cnn1.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "model_cnn1.add(layers.MaxPooling2D(2))\n",
    "model_cnn1.add(layers.Conv2D(64, 3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-mjX5YNyqVd"
   },
   "source": [
    "Passons maintenant à la partie dense. À la dernière couche, les données sont sous la forme d'une tableau et non d'une liste. Il faut donc de nouveau utiliser la couche `Flatten` comme toute à l'heure!\n",
    "\n",
    "**Instruction :**\n",
    "1. Ajoutez une couche `Flatten`.\n",
    "2. Rajoutez une couche `Dense` avec 64 noeuds et la fonction d'activation `relu`.\n",
    "2. Rajoutez une couche `Dense` avec 10 noeuds et la fonction d'activation `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qlKUw4FyqVf"
   },
   "outputs": [],
   "source": [
    "model_cnn1.add(layers.Flatten())\n",
    "model_cnn1.add(layers.Dense(64, activation='relu'))\n",
    "model_cnn1.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RNMxspCyqVk"
   },
   "source": [
    "#### Préparation du modèle à l'entrainement\n",
    "\n",
    "**Instructions :**\n",
    "1. Visualisez votre modèle avec `summary()`\n",
    "2. Compiler votre modèle avec la fonction `compile()` et les mêmes paramètres que la partie prédédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffqktUM4yqVk"
   },
   "outputs": [],
   "source": [
    "model_cnn1.summary()\n",
    "model_cnn1.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction :** Entrainez votre modèle avec la fonction `fit()` et les mêmes paramètres que précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z-ArfGJyqVt"
   },
   "outputs": [],
   "source": [
    "history = model_cnn1.fit(train_images, train_labels,\n",
    "                         epochs=5,\n",
    "                         validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1jdSxE9yqV0"
   },
   "source": [
    "De la même façon que tout à l'heure, regardons les performances de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgzykQKY1rJt"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eCbaEFsyqV5"
   },
   "source": [
    "Regardez également la matrice de confusion du modèle de la même façon dans la partie précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jse3MS0vyqV-"
   },
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(model_cnn1(test_images), axis=1)\n",
    "cm = confusion_matrix(test_labels, pred_labels, normalize='true')\n",
    "show_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1nEuX2cyqWD"
   },
   "source": [
    "On voit que la diagonale est beaucoup plus marqué qu'avant.\n",
    "\n",
    "#### Allez plus loin\n",
    "\n",
    "Vous pouvez essayez de créer un modèle encore meilleur en changeant le nombre de couches, ainsi que les paramètres ... On me murmure à l'oreille que certains 2As ont même atteint plus de 85% !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGWD4UwcyqWE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llgMhj5x1rJ0"
   },
   "source": [
    "## Auto-encodeurs et applications diverses\n",
    "\n",
    "Dans la partie précédente, on devinait directement à partir de l'image qu'elle était sa classe. Maintenant, nous allons supposer que nous ne connaissons pas les classes des images, et nous allons essayer de trouver des similarités entre toutes les images. Pour se faire, nous allons utiliser des auto-encodeurs.\n",
    "\n",
    "\n",
    "## Un auto-enco-quoi ?\n",
    "\n",
    "Les **auto-encodeurs** sont une famille de réseaux de neurones particuliers qui ont pour premier objectif de réduire la dimension de l'espace des données qui nous intéressent. Il se décomposent en deux parties :\n",
    "\n",
    "*   Un encodeur qui doit apprendre la représentation des données d'entrée qui permet de passer de la dimension initiale à la dimension réduite.\n",
    "*   Un décodeur qui doit reproduire l'entrée le plus fidèlement possible à partir de la représentation que donne l'encodeur.\n",
    "\n",
    "On retrouve donc un structure caractéristique en \"goulot d'étranglement\" où les couches aux extrémités sont de la même dimension que la taille des données à caractériser et où la couche centrale est de la dimension de réduction souhaitée qui contiendra la représentation des entrées.\n",
    "\n",
    "![](https://www.researchgate.net/publication/318204554/figure/fig1/AS:512595149770752@1499223615487/Autoencoder-architecture.png)\n",
    "\n",
    "Pour entrainer ce réseau, on va seulement utilisée les données d'entrées : pas besoin donc de labéliser les données, on laisse la descente de gradient faire tout le travail : il s'agit d'un apprentissage non-supervisé.\n",
    "\n",
    "## Vecteurs latents\n",
    "\n",
    "Les vecteurs que l'on obtient après compression par l'auto-encodeur sont appelés **vecteurs latents**, ou **caractéristiques profondes**, ou **deep features**.\n",
    "Ces termes ne sont pas réservés aux auto-encodeurs, il s'agit de n'importe quels vecteurs de dimension réduite provenant d'un réseau profond.\n",
    "\n",
    "Ces vecteurs sont très utiles car ils sont de petites dimensions mais sont porteurs d'information sémantique. Autrement dit, ils \"encodent\" les données d'entrée du réseau, et sont un condensé des informations importantes.\n",
    "\n",
    "> Prenons un exemple.\n",
    "> Vous entrainez un réseau à reconnaitre des voitures (donc supervisé). Une fois que votre réseau est efficace sur des voitures, vous le coupez quelques couches avant la fin. Vous obtenez un réseau qui prend en entrée des images et donne en sortie un vecteur de petite dimension, disons 64. Comme votre réseau aura appris à extraire les informations utiles pour reconnaitre une voiture (roues, forme, etc), ces vecteurs comporteront toutes les informations nécessaires à reconnaitre des voitures.\n",
    "Ensuite, si vous donnez à ce réseau des images de motos et de camions, une simple comparaison des vecteurs de sortie pourra indiquer si il s'agit d'une moto ou d'un camion. En revanche, il sera évidemment toujours peu efficace pour reconnaitre des objets éloignés des voitures, comme des visages par exemple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y40pysmG1rJ2"
   },
   "source": [
    "## Un premier exemple d'auto-encodeur\n",
    "\n",
    "On a vu précédemment que les couches denses n'étaient pas très adaptées pour manipuler les images. Mais pour bien comprendre le concept de l'auto-encodeur, on va commencer avec ça.\n",
    "\n",
    "Si vous avez bien suivi, un auto-encodeur est composé de deux modèles : un encodeur, et un décodeur. Ce dernier est souvent un symétrique de l'encodeur.\n",
    "\n",
    "On va donc créer une classe `AutoencoderDense` qui contiendra ces deux modèles.\n",
    "\n",
    "*Note :* Pas d'inquiétude si vous n'êtes pas familié avec les classes, vous en manipulez déjà depuis toute à l'heure sans le savoir, et ici la structure sera déjà présente, vous n'aurez qu'à compléter les parties manquantes.\n",
    "\n",
    "`latent_dim` est la dimension de la couche latente de l'auto-encodeur, c'est-à-dire la couche au milieu de celui-ci.\n",
    "\n",
    "**Instructions (Encodeur) :**\n",
    "1. Rajoutez une couche `Flatten` en précisant `input_shape` à `encoder`. Les données d'entrée sont les images.\n",
    "2. Rajoutez une couche `Dense` avec 1024 noeuds et la fonction d'activation `relu` à `encoder`.\n",
    "3. Rajoutez une couche `Dense` avec `latent_dim` noeuds et la fonction d'activation `relu` à `encoder`.\n",
    "\n",
    "\n",
    "**Instructions (Décodeur) :**\n",
    "1. Rajoutez une couche `Dense` avec 1024 noeuds, la fonction d'activation `relu` et `input_shape` à `decoder`.\n",
    "\n",
    "   Les données d'entrées sont les valeurs de la couche latente, qui est de dimension `latent_dim`.\n",
    "   \n",
    "   **Attention :** `input_shape` prend en argument un tuple, donc méfiez-vous ... Affichez (45) et (45,) si vous ne voyez pas le problème.\n",
    "\n",
    "\n",
    "2. Rajoutez une couche `Dense` avec 32 * 32 * 3 noeuds (ie le nombre de valeurs d'entrée) et la fonction d'activation `sigmoid` à `decoder`.\n",
    "\n",
    "   À votre avis, pourquoi utilise-t-on la fonction d'activation sigmoid plutôt que relu pour cette dernière couche ? Je vous invite à regarder sur Google à quoi ressemble ces deux fonctions, et à réfléchir à ce qu'on prévoit avec cette dernière couche.\n",
    "   \n",
    "   \n",
    "3. Rajoutez une couche `Reshape((32, 32, 3))` à `decoder` afin de mettre tout ça sous la forme d'une image couleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWa2EjZn1rJ3"
   },
   "outputs": [],
   "source": [
    "class AutoencoderDense(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Définition de l'encodeur\n",
    "        encoder = keras.Sequential(name='encoder')\n",
    "        # À remplir\n",
    "        \n",
    "        # Définition du décodeur\n",
    "        decoder = keras.Sequential(name='decoder')\n",
    "        # À remplir\n",
    "        \n",
    "        # Sauvegarde de l'encodeur et du décodeur\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54Fc_93fyqWS"
   },
   "source": [
    "Vous venez de définir la classe de l'auto-encodeur, il ne reste plus qu'à en créer un !\n",
    "\n",
    "**Instructions :**\n",
    "1. Créez un autoencodeur avec : `autoencoder_dense = AutoencoderDense(latent_dim)`.\n",
    "\n",
    "\n",
    "2. Affichez sa description avec : `autoencoder_dense.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7gq1PFjyqWT"
   },
   "outputs": [],
   "source": [
    "latent_dim = 512\n",
    "autoencoder_dense = AutoencoderDense(latent_dim)\n",
    "autoencoder_dense.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfvkyNLcyqWZ"
   },
   "source": [
    "Pour ceux qui ne sont pas familier avec les classes, vous remarquez que vous venez de créer votre première classe ! ;)\n",
    "\n",
    "Il ne reste plus qu'à le compiler, et à l'entrainer !\n",
    "\n",
    "**Instructions :**\n",
    "1. Compilez `autoencoder_dense` avec la fonction `compile` et les paramètres suivants : `'adam'` comme optimiseur et `losses.MeanSquaredError()` comme loss.\n",
    "\n",
    "\n",
    "2. Entrainez `autoencoder_dense` avec la fonction `fit`.\n",
    "\n",
    "   **Attention**, les données de sortie ne sont plus les mêmes ! En effet, on cherche à reconstruire l'image en entrée après avoir réduit sa dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2RIEcXpyqWa"
   },
   "outputs": [],
   "source": [
    "autoencoder_dense.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_dense.fit(train_images, train_images,\n",
    "                    epochs=5,\n",
    "                    validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0kfWmg5yqWg"
   },
   "source": [
    "Vous disposez maintenant d'un modèle capable, en théorie, de copier les images qu'on lui passe en entrée ... Regardons un peu ça.\n",
    "\n",
    "Le code qui suit prend `n` indices au hasard, et affiche l'image originale et l'image en sortie de l'auto-encodeur correspondant à chaque indice.\n",
    "\n",
    "Vous pouvez voir qu'on accède à l'encodeur et au décodeur de notre modèle à l'aide de `autoencoder_dense.encoder` et `autoencoder_dense.decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GX_aa6OyqWg"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "indices = np.random.choice(len(test_images), n)\n",
    "\n",
    "encoded_imgs = autoencoder_dense.encoder(test_images[indices]).numpy()\n",
    "decoded_imgs = autoencoder_dense.decoder(encoded_imgs).numpy()\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[indices][i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6hfwB1qyqWj"
   },
   "source": [
    "Bilan ? C'est un peu flou tout ça. On pouvait s'y attendre. Notre réseau est composé uniquement de couches denses et le dataset est assez compliqué, notre modèle est donc très mal adapté.\n",
    "\n",
    "C'est pourquoi nous allons utilisé des CNNs ! Après la partie précédente, vous avez au moins une vague idée de ce que c'est. L'essentiel à retenir est que les CNNs sont beaucoup plus efficaces pour traiter des images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpFli5kv1rJ-"
   },
   "source": [
    "## Un deuxième exemple\n",
    "\n",
    "De la même façon que précédemment, nous allons créer une classe `AutoencoderCNN`.\n",
    "\n",
    "**Instructions (Encodeur) :**\n",
    "1. Rajoutez une couche `Conv2D` de 64 filtres, de taille 3, de fonction d'activation `'relu'` et de `padding='same'` en précisant `input_shape` à `encoder`. \n",
    "2. Rajoutez une couche `MaxPooling2D` de taille 2 à `encoder`.\n",
    "3. Rajoutez une couche `Conv2D` de 32 filtres, de taille 3, de fonction d'activation `'relu'` et de `padding='same'` à `encoder`. \n",
    "4. Rajoutez une couche `MaxPooling2D` de taille 2 à `encoder`.\n",
    "3. Rajoutez une couche `Flatten` à `encoder`.\n",
    "3. Rajoutez une couche `Dense` avec `latent_dim` noeuds et la fonction d'activation `relu` à `encoder`.\n",
    "\n",
    "\n",
    "**Instructions (Décodeur) :**\n",
    "1. Rajoutez une couche `Reshape((8, 8, 16))` en précisant `input_shape` à `decoder`. \n",
    "2. Rajoutez une couche `UpSampling2D` de taille 2 à `decoder`. Cette couche permet de doubler la longueur et largueur de l'image. Ce qui à un effet un peu \"inverse\" au MaxPooling.\n",
    "3. Rajoutez une couche `Conv2D` de 16 filtres, de taille 3, de fonction d'activation `'relu'` et de `padding='same'` à `decoder`. \n",
    "4. Rajoutez une couche `UpSampling2D` de taille 2 à `decoder`.\n",
    "5. Rajoutez une couche `Conv2D` de 32 filtres, de taille 3, de fonction d'activation `'relu'` et de `padding='same'` à `decoder`. \n",
    "6. Rajoutez une couche `Conv2D` de 3 filtres, de taille 3, de fonction d'activation `'sigmoid'` et de `padding='same'` à `decoder`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9VBK3d51rJ_"
   },
   "outputs": [],
   "source": [
    "latent_dim = 8 * 8 * 16\n",
    "\n",
    "class AutoencoderCNN(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Définition de l'encodeur\n",
    "        encoder = keras.Sequential(name='encoder')\n",
    "        encoder.add(layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "        encoder.add(layers.MaxPooling2D(2))\n",
    "        encoder.add(layers.Conv2D(16, 3, activation='relu', padding='same'))\n",
    "        encoder.add(layers.MaxPooling2D(2))\n",
    "        \n",
    "        encoder.add(layers.Flatten()) # Préparation couche latente\n",
    "        encoder.add(layers.Dense(latent_dim, activation='relu')) # Couche latente\n",
    "        \n",
    "        # Définition du décodeur\n",
    "        decoder = keras.Sequential(name='decoder')\n",
    "        decoder.add(layers.Reshape((8, 8, 16), input_shape=(latent_dim,)))\n",
    "        decoder.add(layers.UpSampling2D(2))\n",
    "        decoder.add(layers.Conv2D(16, 3, activation='relu', padding='same'))\n",
    "        decoder.add(layers.UpSampling2D(2))\n",
    "        decoder.add(layers.Conv2D(32, 3, activation='relu', padding='same'))\n",
    "        decoder.add(layers.Conv2D(3, 3, activation='sigmoid', padding='same'))\n",
    "        \n",
    "        # Sauvegarde de l'encodeur et du décodeur\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtudBdul1rKE"
   },
   "source": [
    "Il ne reste plus qu'à le créer, le compiler et l'entrainer maintenant.\n",
    "\n",
    "**Instructions :**\n",
    "1. Créez un autoencodeur avec : `autoencoder_cnn = AutoencoderCNN(latent_dim)`.\n",
    "2. Compilez `autoencoder_cnn` avec a fonction `compile` et les paramètres suivants : `'adam'` comme optimiseur et `losses.MeanSquaredError()` comme loss.\n",
    "\n",
    "\n",
    "3. Entrainez `autoencoder_cnn` avec la fonction `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8CudU931rKF"
   },
   "outputs": [],
   "source": [
    "autoencoder_cnn = AutoencoderCNN(latent_dim)\n",
    "autoencoder_cnn.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_cnn.fit(train_images, train_images,\n",
    "                    epochs=3,\n",
    "                    validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Eg86Zu1rKK"
   },
   "source": [
    "Regardons maintenant ce que le modèle réussit à faire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3V-UgOt1rKN"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "indices = np.random.choice(len(test_images), n)\n",
    "\n",
    "encoded_imgs = autoencoder_cnn.encoder(test_images[indices]).numpy()\n",
    "decoded_imgs = autoencoder_cnn.decoder(encoded_imgs).numpy()\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images[indices][i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QhqgrB3yqW2"
   },
   "source": [
    "C'est un peu mieux, mais ce n'est clairement pas encore parfait  ...\n",
    "\n",
    "Il existe de nombreuses techniques pour améliorer les auto-encodeurs, mais nous n'allons pas nous intéresser à ça dans la suite du TP. Nous allons voir d'autres applications des auto-encodeurs, et d'autres techniques d'apprentissage non-supervisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3TeVZOq1rKS"
   },
   "source": [
    "## Applications diverses d'un auto-encodeur\n",
    "\n",
    "Les auto-encodeurs peuvent servir pour la compression (avec perte), mais pas seulement ! On peut aussi s'en servir pour débruiter des images ou faire de la génération de contenu. Voici quelques démonstrations.\n",
    "\n",
    "On va commencez par définir un auto-encodeur un peu plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w34JJz11rKS"
   },
   "outputs": [],
   "source": [
    "class AutoencoderCNN2(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Définition de l'encodeur\n",
    "        encoder = keras.Sequential([\n",
    "            layers.Conv2D(32, 3, 1, activation='relu', padding='same'), # 32 x 32 x 32\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(32, 3, 2, activation='relu', padding='same'), # 16 x 16 x 32\n",
    "            layers.Conv2D(32, 3, 1, activation='relu', padding='same'), # 16 x 16 x 32\n",
    "            layers.BatchNormalization(),\n",
    "            #layers.Conv2D(32, 3, 2, activation='relu', padding='same'), # 8 x 8 x 32\n",
    "            #layers.Conv2D(32, 3, 1, activation='relu', padding='same'), # 8 x 8 x 32\n",
    "        ], name='encoder')\n",
    "        \n",
    "        # Définition du décodeur\n",
    "        decoder = keras.Sequential([\n",
    "            #layers.UpSampling2D(), # 16 x 16 x 32\n",
    "            #layers.Conv2D(32, 3, 1, activation='relu', padding='same'), # 16 x 16 x 32\n",
    "            layers.UpSampling2D(), # 32 x 32 x 32\n",
    "            layers.Conv2D(32, 3, 1, activation='relu', padding='same'), # 32 x 32 x 32\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(3, 1, 1, activation='sigmoid', padding='same') # 32 x 32 x 3\n",
    "        ], name='decoder')\n",
    "        \n",
    "        # Sauvegarde de l'encodeur et du décodeur\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VKM8_DOyqW-"
   },
   "source": [
    "### Le débruitage\n",
    "\n",
    "On va entrainer notre modèle a prédire les images non-bruités à partir d'images bruitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b7O640fyqW_"
   },
   "outputs": [],
   "source": [
    "ae = AutoencoderCNN2()\n",
    "ae.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peCprGT-yqXD"
   },
   "source": [
    "À chaque époque, nous générons de nouvelles images bruitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Wgl_6vgyqXE"
   },
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "NOISE = 0.3     #  Set to 0 for a regular (non-denoising...) autoencoder\n",
    "for i in range(n_epochs):\n",
    "    noise = np.random.normal(0, NOISE, train_images.shape)\n",
    "    \n",
    "    # Keep value between 0 and 1\n",
    "    imgs = train_images + noise\n",
    "    imgs = np.where(imgs > 1., 1., imgs)\n",
    "    imgs = np.where(imgs < 0., 0., imgs)\n",
    "    ae.fit(imgs, train_images, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krn4TuYayqXH"
   },
   "source": [
    "Voyons voir ce que ça donne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7HgZfr2yqXI"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "indices = np.random.choice(len(test_images), n)\n",
    "\n",
    "NOISE = 0.2\n",
    "imgs = (test_images + np.random.normal(0, NOISE, test_images.shape))[indices]\n",
    "\n",
    "# Keep value between 0 and 1\n",
    "imgs = np.where(imgs > 1., 1., imgs)\n",
    "imgs = np.where(imgs < 0., 0., imgs)\n",
    "\n",
    "denoised_imgs = ae(imgs)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(imgs[i])\n",
    "    plt.title(\"original\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(denoised_imgs[i])\n",
    "    plt.title(\"reconstructed\")\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oho0GDEpyqXO"
   },
   "source": [
    "On voit que cela fonctionne plutôt bien et cela a des applications pratiques dans la vraie vie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgFwN0CqyqXO"
   },
   "source": [
    "### La génération d'image\n",
    "\n",
    "L'idée est la suivante : La couche latente est un vecteur de taille N que le décodeur sait transformer en image, donc si on peut générer des vecteurs aléatoires, et obtenir des nouvelles images, non ?\n",
    "\n",
    "Dans un premier temps, on va déjà récupérer toutes les images avec des bateaux pour savoir exactement quelle classe le modèle essayera de générer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fWRemisyqXP"
   },
   "outputs": [],
   "source": [
    "ind_train = train_labels == 8\n",
    "ind_train = ind_train.reshape((-1,))\n",
    "boats_imgs_train = train_images[ind_train, :, :]\n",
    "\n",
    "ind_test = test_labels == 8\n",
    "ind_test = ind_test.reshape((-1,))\n",
    "boats_imgs_test = test_images[ind_test, :, :]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(boats_imgs_train[i], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wcRO1-YyqXS"
   },
   "source": [
    "Puis, nous allons entrainer un auto-encodeur sur ces magnifiques images de bateaux. Le nombre d'images étant divisé par 10, l'entrainement va allez beaucoup plus vite !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlEyk2rhyqXT"
   },
   "outputs": [],
   "source": [
    "ae_gen = AutoencoderCNN2()\n",
    "ae_gen.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "ae_gen.fit(boats_imgs_train, boats_imgs_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(boats_imgs_test, boats_imgs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT4AEjwPyqXW"
   },
   "source": [
    "Essayons maintenant de générer des vecteurs aléatoires de la taille de la couche latente, et voyons si ça fonctionne ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m91AxerGyqXX"
   },
   "outputs": [],
   "source": [
    "random_vectors = np.random.normal(0, .3, size=(10, 16*16*32))\n",
    "random_vectors = random_vectors.reshape((-1, 16, 16, 32))\n",
    "generated_imgs = ae_gen.decoder(random_vectors)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(generated_imgs[i])\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMCRkNyNyqXa"
   },
   "source": [
    "Bof ... C'est pas très concluant comme test, et c'est normal. En réalité, les 16 x 16 x 32 = 8192 coordonées du vecteur de l'espace latent sont corrélées, on ne peut donc pas mettre des valeurs complétement aléatoires. On va utiliser un algorithme linéaire afin de décorreler les variables de notre espace latent afin de s'assurer de générer des bateaux qui sont \"dans la moyenne\"; il s'agit de l'algorithme PCA (analyse en composante principale, pour les matheux c'est pas très compliqué et on peut l'implémenter soit même, voir sur [wikipédia](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)).\n",
    "\n",
    "L'idée général est de trouver les axes qui expliquent le plus notre nuage de points. Bref, si c'est pas clair, vous le verrez durant le cours de Statistiques & Apprentissages, et un peu dans la partie suivante.\n",
    "\n",
    "Ici, on va utiliser le PCA déjà écrit de la librairie **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6N1ETvRyqXb"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RizCnkPZyqXf"
   },
   "outputs": [],
   "source": [
    "# Génération de notre nuage de points\n",
    "samples_pca = ae_gen.encoder(boats_imgs_test)\n",
    "reshaped_samples_pca = tf.reshape(samples_pca, (-1, 16*16*32)) # On les reshape, car ils sont de la forme 16x16x32\n",
    "\n",
    "N = 100\n",
    "pca = PCA(n_components=N, svd_solver='full')\n",
    "pca.fit(reshaped_samples_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bytLwonHyqXi"
   },
   "source": [
    "**Explication du code ci-dessus :**\n",
    "- Dans un premier temps, on crée notre nuage de points, c'est-à-dire l'ensemble des vecteurs latents issus des images de test. \n",
    "\n",
    "- On crée un objet `PCA`. `n_components` précise le nombre d'axes qui nous intéresse, l'algorithme PCA effectura alors une réduction de dimension sur ces axes.\n",
    "\n",
    "- Finalement, on ajuste les paramètres à partir de notre nuage de points avec la fonction `fit`.\n",
    "\n",
    "Passons enfin à la génération de 49 magnifiques bateaux !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f58vNIwbyqXk"
   },
   "outputs": [],
   "source": [
    "n = 49\n",
    "\n",
    "# On regarde comment varie les vecteurs latents réduits pour ne pas générer des valeurs trop différentes.\n",
    "a = pca.transform(reshaped_samples_pca)\n",
    "random_vectors = np.random.normal(loc=np.mean(a, axis=0), scale=np.std(a, axis=0), size=(n, N))\n",
    "\n",
    "# On transforme les vecteurs en vecteurs valables pour l'espace latent\n",
    "random_vectors = pca.inverse_transform(random_vectors) # Transforme les vecteurs réduits dans l'espace initial\n",
    "random_vectors = random_vectors.reshape((-1, 16, 16, 32))\n",
    "\n",
    "# On transforme les vecteurs de l'espace latent en image\n",
    "decoded_random_images = ae_gen.decoder(random_vectors)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(49):\n",
    "    plt.subplot(7,7,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(decoded_random_images[i], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg-qkSkqyqXo"
   },
   "source": [
    "J'ai peut-être menti ... Ils ne sont pas si magnifiques, mais c'est déjà un début. Le type d'auto-encodeur que nous avons défini n'est pas vraiment fait pour la génération, les auto-encodeurs variationnels sont plus adaptés.\n",
    "\n",
    "Je vous invite à jouer avec la variable `N` qui définit le paramètre `n_components` dans le code ci-dessus et de voir l'impact qu'il a sur les images générées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fozIN64p1rKZ"
   },
   "source": [
    "# Classification non-supervisée : analyse des caractéristiques profondes de l'auto-encodeur et clustering\n",
    "\n",
    "Le but général de cette dernière partie est de vous montrer quelques outils classiques de Machine Learning et d'analyse des données. Dans la continuité de la première partie, l'idée est de pouvoir analyser les vecteurs latents provenants de l'auto-encodeur. En particulier, vous serez capables d'appliquer un clustering non-supervisé sur l'espace latent pour pouvoir retrouver les 10 classes de CIFAR-10, sans jamais utiliser les labels.\n",
    "\n",
    "Nous allons voir dans cette partie :\n",
    "- l'importance de la normalisation des données\n",
    "- la visualisation par t-SNE\n",
    "- la réduction de dimensions par ACP (Ou PCA en anglais)\n",
    "- le clustering par K-means\n",
    "- le clustering par agglomération\n",
    "\n",
    "Ensuite, vous pourrez appliquer un clustering aux données de l'auto-encodeur (sur les images de CIFAR-10). Celui qui aura le meilleur score gagne le TP !\n",
    "\n",
    "## Dataset\n",
    "\n",
    "On va voir chacun de ces outils de Machine Learning en s'entrainant sur le dataset \"Wine\" qui contient 3 classes de différents vins, dont chacun a 13 attributs (principalement les taux de présence de composés chimiques).\n",
    "\n",
    "Commençons par importer le dataset et quelques modules utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vmauXUm1rKZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine()\n",
    "wine_train_data = data['data']\n",
    "wine_train_labels = data['target']\n",
    "\n",
    "print(f\"Les {wine_train_data.shape[0]} vecteurs sont de dimension {wine_train_data.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRtyi0Eb1rKg"
   },
   "source": [
    "## Analyse des données\n",
    "\n",
    "Il est souvent nécessaire de réduire la dimension des données que l'on manipule. Un encodeur (par exemple la première partie d'un auto-encodeur) est déjà un moyen de réduire les dimensions en codant des images dans un espace vectoriel beaucoup plus petit.\n",
    "\n",
    "Un auto-encodeur a le rôle très particulier d'encoder des images (ou autres données structurées), mais il existe d'autres algorithmes qui s'appliquent à des données vectorielles classiques.\n",
    "\n",
    "La réduction de dimension permet à la fois à mieux visualiser les données (par exemple on peut réduire les vecteurs à 2 ou 3 dimensions pour afficher les données sur un graphique), et à la fois de pré-traiter les données. Pré-traiter les données en réduisant le nombre de dimensions est parfois indispensable, en effet, certains algorithmes - en particulier ceux basés sur la distances- supportent mal les très hautes dimensions.\n",
    "\n",
    "> Pour ceux que ca intéresse, vous pouvez regardez la *malédiction de la dimension* : [version simple (Wikipedia)](<https://fr.wikipedia.org/wiki/Fl%C3%A9au_de_la_dimension>) / [version hardcore](<https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.64.2646&rep=rep1&type=pdf>).\n",
    "\n",
    "Dans la suite du TP, nous verrons la transformation t-SNE pour la visualisation, et l'ACP pour le pre-process.\n",
    "\n",
    "**Il est important de noter que les données que l'on va utiliser dans cette partie n'ont que 13 dimensions. C'est minuscule et ça ne nécessite pas de réduire encore plus la dimension, l'ACP est là à titre pédagogique.**\n",
    "\n",
    "**Par contre, la dernière partie du TP ne pourra pas se faire sans utiliser une ACP.**\n",
    "\n",
    "### Visualisation (t-SNE)\n",
    "\n",
    "Le t-SNE (pour t-distributed stochastic neighbor embedding) est un algorithme de réduction de dimension spécialement conçu pour la visualisation de données. L'algorithme tend à maintenir les proximités entre les points depuis l'espace de grande dimension vers l'espace de petite dimension, selon une métrique donnée.\n",
    "\n",
    "> Si vous voulez creuser son fonctionnement (ça demande quelques connaissances en théorie de l'information) : \n",
    "> [version simple](<https://aiaspirant.com/introduction-to-t-distributed-stochastic-neighbor-embeddingt-sne/>) / [version complète](<https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf>).\n",
    "\n",
    "Notez que ca a été inventé par **Geoffrey Hinton**, un monument de l'IA, père de beaucoup d'autres méthodes. Vous allez probablement recroiser son nom souvent.\n",
    "\n",
    "Comme on va le voir, le t-SNE n'est pas déterministe (c'est-à-dire qu'il a une part d'aléatoire dans son exécution), et c'est une chose à ne pas oublier quand vous l'utiliserez.\n",
    "\n",
    "**Instructions :**\n",
    "1. Créez un t-SNE à l'aide de `tsne = TSNE(n_components=..., perplexity=..., n_iter=...)`.\n",
    "   \n",
    "   `n_components` : C'est la dimension dans lequel on réduit l'espace. On veut projeter dans le plan, on prendra donc 2.\n",
    "   \n",
    "   `perplexity`: Est relié au nombre de voisins considérés lors de la réduction de dimension. On prendra 40.\n",
    "   \n",
    "   `n_iter`: Nombre d'itérations de l'optimisation.\n",
    " \n",
    " \n",
    "2. Paramétrez votre modèle tout en transformant vos données à l'aide de `tsne_result = tsne.fit_transform(wine_train_data)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Y4ZC8J21rKl"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(wine_train_data)\n",
    "\n",
    "tsne_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKjlBG2qyqXw"
   },
   "source": [
    "Au vu de la forme du résultat, les données ont bien été transformée en une liste de points 2D. On peut donc les représenter dans le plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FituxL5CyqXx"
   },
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Transformation t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeMJLdZo1rKr"
   },
   "source": [
    "Le t-SNE n'est pas déterministe. Vous pouvez le vérifier en lançant plusieurs fois la réduction, vous n'aurez jamais le même résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbF6B1LFyjB1"
   },
   "source": [
    "### Normalisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSp0K3Jxy5tE"
   },
   "source": [
    "Beaucoup d'algorithmes se basent sur les distances entre les données. Or la qualité des calculs des distances dépend fortement de la normalisation des axes. Par exemple, un axe qui a des valeurs entre 0 et 100 aura une influence beaucoup plus forte car ses valeurs sont beaucoup plus grandes que celles prises dans un axe ayant des valeurs entre 0 et 1, et on verrait une distorsion artificielle le long de l'axe non normalisé.\n",
    "\n",
    "Il y a principalement deux façons de normaliser, soit on ramène toutes les données entre un min et un max (`MinMaxScaler`), soit on normalise les données pour que chaque axe soit centré et réduit (`StandardScaler`).\n",
    "\n",
    "**Instruction :** Afficher le min et le max des données sur chaque axe des données du dataset \"Wine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ev22rYrty3Kk"
   },
   "outputs": [],
   "source": [
    "for axe in range(wine_train_data.shape[1]):\n",
    "    extract = wine_train_data[:, [axe]]\n",
    "    print(f\"axe {axe} - min {min(extract)} - max {max(extract)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5_8PpI5zHQz"
   },
   "source": [
    "Les différentes composantes des vecteurs d'entrainement n'ont pas du tout les mêmes plages de valeurs. Pour vous entrainer à manipuler les transformateurs de normalisation, affichez en dessous trois transformations t-SNE, une avec les données brutes, une avec une normalisation MinMax et une avec une normalisation gaussienne.\n",
    "\n",
    "**Instructions :**\n",
    "\n",
    "1. Normalisez vos données d'entrainement avec :\n",
    "   - `StandardScaler`. Stockez les données normalisées dans `wine_train_data_norm`.\n",
    "   - `MinMaxScaler`. Stockez les données normalisées dans `wine_train_data_minmax`.\n",
    "   \n",
    "\n",
    "2. Affichez trois transformations t-SNE :\n",
    "   - Une avec les données brutes. Stockez le résultant dans `tsne_results_raw`.\n",
    "   - Une avec une normalisation MinMax. Stockez le résultant dans `tsne_results_minmax`.\n",
    "   - Une avec une normalisation gaussienne (StandardScaler). Stockez le résultant dans `tsne_results_norm`.\n",
    "\n",
    "*Note :* Les normalisations se créent avec `MinMaxScaler()` et `StandardScaler()`, et vous transformez les données de la même manière qu'avec la transformation t-SNE, c'est-à-dire avec la fonction `fit_transform(<data>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsM_r5eDz7XO"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wine_train_data_norm = scaler.fit_transform(wine_train_data)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "wine_train_data_minmax = scaler.fit_transform(wine_train_data)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=40, n_iter=300)\n",
    "tsne_results_raw = tsne.fit_transform(wine_train_data)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=40, n_iter=300)\n",
    "tsne_results_norm = tsne.fit_transform(wine_train_data_norm)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=40, n_iter=300)\n",
    "tsne_results_minmax = tsne.fit_transform(wine_train_data_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons les différents nuages de points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "plt.scatter(tsne_results_raw[:,0], tsne_results_raw[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"données non normées\")\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "plt.scatter(tsne_results_norm[:,0], tsne_results_norm[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"données normées minmax\")\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "plt.scatter(tsne_results_minmax[:,0], tsne_results_minmax[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"données normées standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBvdMw2oyqYH"
   },
   "source": [
    "On observe que la distinction entre les nuages de points est beaucoup plus clair pour les données normalisées que les données brutes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fnqdtAq1rKs"
   },
   "source": [
    "### Réduction de dimension (ACP/PCA)\n",
    "\n",
    "L'**Analyse en Composantes Principales** est une autre méthode de réduction de dimension. L'ACP repose sur un principe intuitif : on recherche les directions dans lesquelles les données s'étalent le plus.\n",
    "\n",
    "On vous encourage à regarder l'idée générale sur les liens ci-contre, ca se comprend très vite. \n",
    "[version simple (wiki)](<https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales>) / [version complète](<https://www.researchgate.net/publication/309165405_Principal_component_analysis_-_a_tutorial>).\n",
    "\n",
    "C'est la technique qui est de très loin la plus utilisée pour la réduction de dimension. Elle permet notamment de savoir combien d'information on perd lorsque l'on réduit la dimension. Elle a beaucoup d'autres avantages qui seront détaillés dans la suite.\n",
    "\n",
    "*Note :* Vous l'avez déjà vu à l'oeuvre dans la partie génération avec l'auto-encodeur.\n",
    "\n",
    "---\n",
    "\n",
    "`PCA(n_components=None, whiten=True, svd_solver='svd')` :\n",
    "\n",
    "- `n_components` :\n",
    "   - Si vous mettez un entier, alors ce sera la dimension de l'espace réduit.\n",
    "   - Si vous mettez une valeur entre 0 et 1, l'algorithme prendra autant de composants qu'il faut, afin d'expliquer ce pourcentage de variance.\n",
    "   \n",
    "**Instructions :**\n",
    "1. Faites un PCA avec 13 composants que vous stockerait dans `pca`.\n",
    "2. Transformez vos données avec `pca.fit_transform(<data>)` comme précédémment, vous stockerait le résultat dans `pca_results`. **Attention**, on travaillera avec les données normaliséss à partir de maintenant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoF1O0aR1rKt"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 13\n",
    "pca = PCA(n_components=n_components, svd_solver='full', whiten=True)\n",
    "pca_results = pca.fit_transform(wine_train_data_norm)\n",
    "\n",
    "\n",
    "# Affichage\n",
    "components = range(1, n_components+1)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "plt.show()\n",
    "plt.figure(figsize=(16,7))\n",
    "\n",
    "# Variance expliquée\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.plot(components, explained_variance)\n",
    "plt.xlabel(\"composantes\")\n",
    "plt.ylabel(\"proportion de variance expliquée\")\n",
    "plt.title(\"Variance expliquée par composante\")\n",
    "\n",
    "# Variance expliquée commulée\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.plot(components, np.cumsum(explained_variance))\n",
    "print('\\nExplained variation per principal component: \\n {} \\n'.format(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"composantes\")\n",
    "plt.ylabel(\"proportion de variance expliquée cumulée\")\n",
    "plt.title(\"Variance expliquée cumulée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEmIsInt1rK0"
   },
   "source": [
    "Le graphe précédent montre la proportion de variance expliquée sur chacune des composantes principales. Intuitivement, il s'agit de la proportion d'information contenue dans chacune des composantes.\n",
    "Suivant vos besoins, vous pouvez donc garder les composantes principales qui vous sont utiles.\n",
    "En ce qui concerne les données que l'on manipule ici, on remarque que 6 dimensions sont nécessaires pour expliquer plus de 90% de la variance.\n",
    "\n",
    "Les 2 premières composantes expliquent presque 50% de la variance, on peut tracer le nuage de points associé.\n",
    "\n",
    "**Instructions :** Réeffectuez un PCA, mais avec seulement 2 composantes, et stockez le résultat dans `pca_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn2SyV261rK1"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "pca_results = pca.fit_transform(wine_train_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ux8MvHyqYh"
   },
   "source": [
    "Affichons tout ça !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UAKlaZMyqYh"
   },
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Les deux premières composantes de l'ACP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iarWZH9e1rK_"
   },
   "source": [
    "#### Pourquoi une ACP, pourquoi un t-SNE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3946NSGW1rLA"
   },
   "source": [
    "On l'a vu, le t-SNE a l'air d'être efficace pour réduire les dimensions, alors pourquoi on utilise une ACP pour le traitement réel des données ? \n",
    "\n",
    "C'est parce-que la transformation t-SNE souffre de quelques points noirs :\n",
    "\n",
    "- Le t-SNE, comme on l'a vu, n'est pas déterministe. La sortie change à chaque exécution, ce qui peut poser problème pour la manipulation des données après la transformation. L'ACP par contre fournit des résultats déterministes.\n",
    "\n",
    "\n",
    "- Le t-SNE se base sur les relations entre les points voisins mais ne permet pas toujours de visualiser les tendances globales. Vous aurez toujours des données bien étalées dans l'espace alors que les données d'entrée ne le sont pas forcément. C'est aussi ce qui rend le t-SNE parfois très utile pour visualiser les données.\n",
    "\n",
    "\n",
    "- Les composantes principales en sortie d'ACP portent en elles-mêmes une signification puisqu'elles se décomposent selon des composantes d'entrée. Si les données d'entrée ont un sens, alors les sorties auront un sens.\n",
    "> Par exemple, si on veut prédire le prix d'une maison, une ACP nous donne des informations du type : \"La composante principale qui explique le prix d'une maison est composée à 20% de sa localisation, à 30% de sa surface, ...\".\n",
    "\n",
    "\n",
    "- L'ACP permet d'obtenir les matrices de passage d'un espace à l'autre qui peuvent être utilisées pour projeter facilement de nouvelles données dans l'espace réduit.\n",
    "\n",
    "\n",
    "- L'ACP sert de base a beaucoup d'autres algorithmes, qui sont utiles pour des cas particuliers ou des cas extrêmes (peu de données, données mal réparties, etc.).\n",
    "\n",
    "**Note :** Le dataset sur le vin est trop simple pour mettre en évidence tout ceci. C'est pourquoi nous allons utiliser le dataset *breast cancer wisconsin* de dimension 30 et à 2 classes (positif ou négatif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRvv03sh1rLC"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data_cancer = load_breast_cancer()\n",
    "train_data_cancer = data_cancer['data']\n",
    "train_labels_cancer = data_cancer['target']\n",
    "\n",
    "scaler_cancer = StandardScaler()\n",
    "train_data_cancer = scaler.fit_transform(train_data_cancer)\n",
    "\n",
    "pca_cancer = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "pca_results_cancer = pca.fit_transform(train_data_cancer)\n",
    "\n",
    "tsne_cancer = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_cancer = tsne.fit_transform(train_data_cancer)\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.title(\"Transformation t-SNE\")\n",
    "plt.scatter(tsne_results_cancer[:,0], tsne_results_cancer[:,1], c=train_labels_cancer, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_results_cancer[:,0], pca_results_cancer[:,1], c=train_labels_cancer, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Transformation par ACP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H28aR5Z1rLB"
   },
   "source": [
    "En comparant les deux algorithmes, on remarque l'étalement provoqué par le t-SNE. C'est utile pour bien visualiser les données mais on perd l'information qui concerne les tendances globales des données. L'ACP donne des nuages de points plus denses ce qui les rend moins simple à visualiser, mais la tendance globale du nuage de point est bien visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRaTMJ4O1rLH"
   },
   "source": [
    "#### Quelques autres algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4nnsDcA1rLI"
   },
   "source": [
    "D'autres algorithmes permettent la réduction de dimension dans des cas particuliers :\n",
    "\n",
    "- La PLS (ou régression des moindres carrés partiels) est équivalente à l'ACP mais elle est supervisée. Ça permet de chercher les composantes principales qui permettent le mieux de distinguer les classes (qui sont donc connues).\n",
    "\n",
    "\n",
    "- L'algorithme FCA analyse les relations entre deux variable qualitatives. C'est donc une ACP pour des données d'entrée non numériques, et pour lesquelles on ne peut pas facilement définir de distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq5kpEof1rLZ"
   },
   "source": [
    "## Classification non-supervisée (clustering)\n",
    "\n",
    "### K-means\n",
    "\n",
    "Le K-means est un algorithme de clustering. Il fonctionne donc de manière non supervisé et se base sur une distance pour comparer les vecteurs.\n",
    "\n",
    "> Le K-means est un algo itératif. À l'initialisation, il prend k points répartis aléatoirement (que nous appelerons les *centroïdes*). Chaque itération comporte deux étapes :\n",
    "- On attribue chaque point des données à un des *centroïdes*. Dans sa forme basique, chaque donnée est attribuée au *centroïde* qui est le plus proche.\n",
    "- Pour chaque *centroïde*, on regroupe tous les points qui lui sont attribués, ceux-ci forment un cluster ; puis chaque *centroïde* est mis à jour comme étant le barycentre de son nouveau cluster.\n",
    "\n",
    ">  Ces étapes sont répétées jusqu'à l'arrêt par un critère de convergence.\n",
    "\n",
    "<img src=\"https://stanford.edu/~cpiech/cs221/img/kmeansViz.png\" width=\"440\" height=\"260\" align=\"center\"/>\n",
    "\n",
    "Pour avoir plus d'infos sur le K-means : [Anas Al-Masri : How does K-means work ?](<https://towardsdatascience.com/how-does-k-means-clustering-in-machine-learning-work-fdaaaf5acfa0>)\n",
    "\n",
    "\n",
    "Il a l'avantage d'être simple et rapide, mais nécessite de donner le nombre de clusters k, et c'est une information que l'on n'a pas toujours. De plus, le choix des valeurs initiales des *centroïdes* influent beaucoup sur le résultat.\n",
    "\n",
    "La solution classique pour pallier ce problème consiste à initialiser les *centroïdes* aléatoirement et lancer l'algorithme plusieurs fois, et avec différentes valeurs de k. Il existe aussi une variante kmeans++ qui intialise les *centroïdes* de manière à optimiser le résultat.\n",
    "\n",
    "---\n",
    "\n",
    "`KMeans(n_clusters=3, random_state=0)`\n",
    "\n",
    "- `n_clusters` : Le nombre de cluster. 3 pour le dataset sur le vin.\n",
    "- `random_state` : Lui donner une valeur entière permet d'avoir la même initialisation des centroïdes, et donc toujours le même résultat à chaque éxécution. Si vous ne voulez pas, vous pouvez mettre `None` à la place.\n",
    "\n",
    "**Instruction :**\n",
    "1. Créez un KMeans que vous stockerait dans `kmeans`.\n",
    "2. Entrainez le modèle que vous venez de créer avec la fonction `fit(<data>)` où `<data>` sera les données normalisées sur le vin.\n",
    "3. Faites ensuite des prédictions avec la fonction `predict(<data>)` que vous stockerez dans `kmeans_wine_predict`.\n",
    "\n",
    "*Note :* Les points 2. et 3. peuvent être combinés à l'aide de la fonction `fit_predict(<data>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqCKKjRz1rLZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(wine_train_data_norm)\n",
    "kmeans_wine_predict = kmeans.predict(wine_train_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kzz50rh4yqYt"
   },
   "source": [
    "Voyons voir si l'algorithme KMeans s'en sort bien ... On va regardez ça en réaffichant le nuage de points du PCA, mais en mettant les vrais labels pour l'image de gauche, et le résultat du KMeans à droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21rqZTajyqYu"
   },
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Données annotées\")\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=kmeans_wine_predict, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Predictions du K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN840igryqYx"
   },
   "source": [
    "On voit que cela fonctionne très bien dans notre cas !\n",
    "\n",
    "**Instructions :** Tracez la matrice de confusion du résultat du KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeRPH5cyyqYy"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(wine_train_labels, kmeans_wine_predict, normalize='true')\n",
    "show_confusion_matrix(cm, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8q264iD1rLe"
   },
   "source": [
    "### Clustering agglomératif\n",
    "\n",
    "Le clustering agglomeratif consiste à constuire un arbre de similitude entre les données.\n",
    "Dans sa forme la plus simple, il s'agit de réunir itérativement les données les plus proches.\n",
    "\n",
    "> En deux mots. On initialise en considèrant chacun des points comme un cluster ayant un unique point. Ensuite à chaque étape on calcule les barycentres de chacun des clusters, et on regroupe les clusters dont les barycentres sont les plus proches.\n",
    "\n",
    "<img src=\"https://cedric.cnam.fr/vertigo/Cours/RCP216/_images/hierarchicalClustering.png\" width=\"440\" height=\"160\" align=\"center\"/>\n",
    "\n",
    "Pour en savoir plus : \n",
    "[version simple (wiki)](<https://fr.wikipedia.org/wiki/Regroupement_hi%C3%A9rarchique>) / [version complète]().\n",
    "\n",
    "Le grand avantage de cette technique c'est qu'elle ne nécessite pas de connaitre le nombre de clusters en avance. On peut lancer l'algorithme et couper l'arbre au nombre de cluster qui nous convient ou qui donne le meilleurs score.\n",
    "\n",
    "Il existe différentes façons de regrouper les clusters, et différentes variantes. Par exemple, on peut aussi considérer tous nos points comme un seul grand cluster, puis le diviser itérativement de manière optimale.\n",
    "\n",
    "Une implémentation du clustering agglomératif est disponible dans sci-kit :\n",
    "`AgglomerativeClustering()`\n",
    "\n",
    "- `.set_params(n_clusters=3)` est une méthode pour définir le nombre de clusters (nécessaire pour `predict`)\n",
    "- les fonctions `fit()`, `predict()` et `fit_predict()` sont disponibles comme pour tous les transformateurs.\n",
    "\n",
    "**Instructions :**\n",
    "1. Créez une instance du clustering agglomératif.\n",
    "2. Réglez le nombre de clusters.\n",
    "3. Faites ensuite des prédictions avec la fonction `fit_predict(<data>)` que vous stockerez dans la variable `ac_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HvTDGV91rLf"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "AC = AgglomerativeClustering()\n",
    "AC.set_params(n_clusters=3)\n",
    "ac_prediction = AC.fit_predict(wine_train_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5StmXBPIyqY7"
   },
   "source": [
    "Affichons notre prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmklAo41yqY7"
   },
   "outputs": [],
   "source": [
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=wine_train_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Données annotées\")\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_results[:,0], pca_results[:,1], c=ac_prediction, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"Predictions de l'AC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BVrIig0K10S"
   },
   "source": [
    "La bbilothèque Scikit ne permet pas de visualiser le clustering hiérarchique. Ci-dessous on utilise Scipy pour afficher le dendogramme. On voit qu'il est assez facile de couper l'arbre à la bonne hauteur pour choisir le nombre de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkEh5u5FJc_c"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "hc_complete = linkage(wine_train_data_norm, \"complete\")\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('donnée')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    hc_complete,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjBG-Mii1rLl"
   },
   "source": [
    "### Score NMI\n",
    "\n",
    "Trouver une bonne métrique pour mesurer la qualité d'un clustering non supervisé n'est pas si simple. En effet, la plupart des métriques classiques se contentent de comparer les *labels* prédits aux vrais labels. Mais dans notre cas, nous avons deux problèmes :\n",
    "\n",
    "  - En non-supervisé sur des données brutes, on n'a pas forcément ces *labels* vrais. Il faut donc se contenter de mesurer à quel point les classes sont denses (**inertie intra-cluster**), et à quel point les différentes classes sont éloignées (**inertie inter-cluster**). Il existe beaucoup de métriques de ce type, on peut citer le **coefficient de Silhouette** ou encore **l'indice de Calinski-Harabasz.**\n",
    "\n",
    "- Lorsque l'on a les labels vrais, ce n'est pas gagné non plus. En effet, si l'algorithme prédit [1, 1, 2, 2, 3, 3] alors que les vraies valeurs sont [3, 3, 1, 1, 2, 2], l'algorithme n'a pas fait d'erreur de clustering, mais une comparaison label à label donnerait un score nul. Il faut donc une métrique qui ne soit pas basée sur le nom des classes. On peut utiliser **l'Adjusted Rand Index (ARI)** ou encore **l'indice de Jacard**, mais pour le TP je vous propose le score **NMI (Normalized Mutual Information)**.\n",
    "\n",
    "\n",
    "\n",
    "> Pour plus d'informations sur les mesures de qualité du clustering, voir \n",
    "[Manimaran : Clustering Evaluation strategies](<https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc>)\n",
    "\n",
    "Voici un exemple de calcul du score NMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW_5kiCH1rLm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score as NMI\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "prediction = kmeans.fit_predict(wine_train_data_norm)\n",
    "\n",
    "print(\"Score NMI :\", NMI(prediction, wine_train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWL64AS0yqZJ"
   },
   "source": [
    "**Instructions :** À vous de jouez, tracez le score en MNI en fonction du nombre de cluster (de 1 à 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x4pprLsyqZK"
   },
   "outputs": [],
   "source": [
    "cluster = []\n",
    "scores = []\n",
    "\n",
    "for k in range(1, 20):\n",
    "  kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "  prediction = kmeans.fit_predict(wine_train_data_norm)\n",
    "  cluster.append(k)\n",
    "  score = NMI(prediction, wine_train_labels)\n",
    "  scores.append(score)\n",
    "  print(f\"\\rk = {k}\", end='')\n",
    "\n",
    "plt.plot(cluster, scores)\n",
    "plt.xlabel(\"nombre de clusters\")\n",
    "plt.ylabel(\"score NMI\")\n",
    "plt.title(\"Qualité du clustering en fonction du nombre de clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRK2t55Z1rLr"
   },
   "source": [
    "On voit que le meilleur clustering se fait pour k=3. Il n'y a rien de très étonnant puisque nous avions trois classes dans les données. Lorsque vous manipulez des données sans connaitre k, cette méthode est un bon moyen de savoir quel est le nombre optimal de classes.\n",
    "\n",
    "### Pipeline sci-kit\n",
    "\n",
    "*Cette sous-partie présente juste une astuce de programmation, peu connue, et très utile.*\n",
    "\n",
    "La manière la plus propre d'utiliser sci-kit lorsque l'on a besoin d'un enchainement de plusieurs transformations, c'est d'utiliser les **pipelines**.\n",
    "\n",
    "Il s'agit d'un regroupement de plusieurs objets *transformers* réunis dans un seul objet ```Pipeline()```.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('nom_1', objet_transformateur_1()),\n",
    "    ('nom_2', objet_transformateur_2()),\n",
    "    etc...\n",
    "])\n",
    "```\n",
    "\n",
    "Pour manipuler votre Pipeline, vous pouvez utiliser les même méthodes que pour les transformateurs : ```.fit()```, ```.predict()```, ```.fit_predict()```, etc.\n",
    "\n",
    "> Vous pouvez aussi réécrire (en subclassing) les classes ```BaseEstimator```, ```TransformerMixin``` pour faire vos propres transformateurs et les inclure dans des pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soASMQ4ASX_q"
   },
   "source": [
    "Pour vous entrainer, essayez de  créer et entrainer une pipeline qui regroupe une ACP et un K-means sur les données ```train_data_viz```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmHudtR-1rLs"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#ceci est un exemple de pipeline, mais en fait il est inutile de faire une ACP ici\n",
    "pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=10, svd_solver='full', whiten=True)),\n",
    "    ('kmeans', KMeans(n_clusters=3, random_state=0)),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(wine_train_data_norm)\n",
    "prediction = pipeline.predict(wine_train_data_norm)\n",
    "\n",
    "print(\"Score NMI :\", NMI(prediction, wine_train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDN9cR9BTGoK"
   },
   "source": [
    "# À vous de jouer en utilisant tout ça !\n",
    "\n",
    "Pour finir le TP, essayez de faire une classification non-supervisée sur CIFAR-10. Celui qui a le score NMI le plus haut gagne le TP.\n",
    "\n",
    "> Indice :\n",
    "> Vous pouvez extraire des caractéristiques grâce à un auto-encodeur, puis les classifier avec un algorithme de clustering de votre choix.\n",
    "\n",
    "N'oubliez pas que vous ne devrez pas utiliser les *labels*, sauf pour le calcul du score NMI.\n",
    "\n",
    "---\n",
    "PS : Si vous êtes motivés, vous pouvez coder un k-means *from scratch* (avec numpy), c'est pas hyper long et ca permet de bien comprendre l'algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vvp9WstlyqZe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

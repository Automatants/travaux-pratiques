{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer\n",
    "Bienvenue à l'un des TPs les plus longues de ce repo. Aujourd'hui on va s'intéresser à l'architecture des transformers.\n",
    "Un transformer est un réseau de neurones qui permet de transformer des séquences de données en d'autres séquences de données. Il est très utilisé dans le domaine du NLP pour faire de la traduction de texte par exemple. C'est aussi l'architecture derrière le fameux ChatGPT, que vous connaissez déjà.\n",
    "\n",
    "Les modèles de génération d'images comme DALLE utilisent aussi des transformers, pour encoder le texte.\n",
    "\n",
    "![chatgpt](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/chatgpt.png)\n",
    "\n",
    "![dalle](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/dalle.png)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Personnellement, je trouve que c'est plus facile à comprendre les transformers en commençant par comprendre comment l'utiliser (comprendre les entrées et les sorties) et ensuite en regardant l'architecture.\n",
    "On va donc commencer par un exemple d'utilisation, puis on va rentrer dans le détail de l'architecture.\n",
    "Donc pour l'instant, on va considérer le transformer comme un black box."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Inférence (ou la phase d'utilisation/prédiction)\n",
    "\n",
    "Pendant l'inférence, le transformer prédit un token (ou un mot, ou un élément de la séquence, c'est la même chose) à la fois. Il prend en compte les tokens précédemment prédits et la séquence d'origine comme entrées pour prédire le token suivant.\n",
    "\n",
    "Considérons l'exemple de la traduction : le transformer utilise à la fois la phrase source et les mots précédemment prédits de la phrase traduite comme entrées afin de générer le mot suivant de la phrase traduite.\n",
    "\n",
    "![blackbox](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/blackbox.png)\n",
    "\n",
    "Pendant la traduction, le transformer prend en entrée la phrase source et la phrase actuellement prédite, puis génère le mot suivant dans la phrase traduite en se basant sur ces deux entrées. Il répète ce processus, en prenant les mots précédemment prédits et la phrase source en entrée à chaque étape, jusqu'à ce que la phrase traduite complète soit générée.\n",
    "\n",
    "Examinons ça avec un exemple détaillé.\n",
    "\n",
    "Supposons que le transformer soit déjà entraîné et qu'on veut qu'il traduise \"I love oranges.\" en français. La phrase traduite qu'on veut obtenir est \"J'aime les oranges\".\n",
    "\n",
    "La phrase source est `[\"I\", \"love\", \"oranges\", \".\"]`.\n",
    "\n",
    "Comme c'est le début de l'inférence, la phrase actuellement prédite est vide, mais on ne peut pas mettre une phrase vide dans le transformer, donc on commence par `[\"<start>\"]` comme phrase actuellement prédite (target sequence).\n",
    "\n",
    "On a donc ces deux séquences en entrées: `[\"I\", \"love\", \"oranges\", \".\"]` et `[\"<start>\"]`.\n",
    "\n",
    "On va tokenizer ensuite ces séquences (convertir chaque mot en un nombre le représentant), par exemple `[\"I\", \"love\", \"oranges\", \".\"]` -> `[10, 35, 20, 49]` et `[\"<start>\"]` -> `[40]`.\n",
    "\n",
    "Le transformer prend ensuite en entrée ces deux séquences, puis génère un vecteur de probabilité sur l'ensemble du vocabulaire, représentant la probabilité de chaque mot possible suivant le token `<start>`.\n",
    "\n",
    "Puisque le transformer est déjà entraîné, le mot avec la probabilité la plus élevée va être `J'`. On ajoute `J'` à la phrase actuellement prédite.\n",
    "\n",
    "![first inference](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/probability1.png)\n",
    "\n",
    "On a donc maintenant `[\"I\", \"love\", \"oranges\", \".\"]` et `[\"<start>\", \"J'\"]`. Avec la tokenisation, on a `[10, 35, 20, 49]` et `[40, 20]`, en supposant que 20 représente `J'`.\n",
    "Remarque qu'on a deux tokeniseurs différents pour l'anglais et le français.\n",
    "\n",
    "Le transformer prend ensuite en entrée ces séquences, puis génère deux vecteurs de probabilité, la première représentant la probabilité du mot possible suivant le token `<start>`, et la seconde celle du mot possible suivant le token `J'`.\n",
    "\n",
    "![second inference](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/probability2.png)\n",
    "\n",
    "Remarque que le premier vecteur de probabilité est identique au précédent. C'est parce que le transformer génère toujours les même vecteurs pour les mêmes entrées même s'il y a des tokens supplémentaires dans la phrase actuellement prédite. Les futurs tokens n'influencent pas les vecteurs de probabilité des tokens précédents. C'est parce qu'il y a un masque qu'on appelle `causal mask` qui empêche les tokens de prêter attention aux futurs tokens; on en discutera plus tard dans la section sur l'architecture du transformer.\n",
    "\n",
    "Puisque on a déjà prédit le mot suivant le token `<start>` à la première étape, on peut ignorer le premier vecteur de probabilité et se concentrer uniquement sur la seconde.\n",
    "\n",
    "\n",
    "Comme le transformer est déjà entraîné, le mot avec la probabilité la plus élevée va être \"aime\".\n",
    "\n",
    "On obtient donc `[\"I\", \"love\", \"oranges\", \".\"]` et `[\"<start>\", \"J'\", \"aime\"]`.\n",
    "\n",
    "On répète ce processus jusqu'à ce que le transformer génère le token `<end>`, indiquant que la séquence générée est complète. Le transformer est entraîneé pour prédire le token <end> de manière appropriée pendant la phase d'entraînement, ce qui nous permet de déterminer quand arrêter la boucle de génération."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice 1: Utilisation d'un transformer pré-entraiîné pour faire de la traduction de texte\n",
    "On va importer un modèle de transformer déjà entraîné et vous allez devoir faire la boucle d'inférence pour traduire la phrase \"I love oranges.\" en français.\n",
    "\n",
    "Téléchargement des libraries: `pip install transformers sentencepiece`.\n",
    "Si vous êtes sur colab, alors lancez cette cellule"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maintenant on va load le modèle. Vous n'avez pas besoin de comprendre la cellule suivante, tout ce qu'il faut retenir c'est que vous avez un objet `transformer` qui contient le modèle et le tokenizer que vous allez utiliser pour faire l'inférence.\n",
    "\n",
    "Je vais vous expliquer comment utiliser le modèle et le tokenizer juste après."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "class MyTransformer:\n",
    "    def __init__(self):\n",
    "        # Load pre-trained model and tokenizer\n",
    "        model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        self.model = MarianMTModel.from_pretrained(model_name)\n",
    "        self.start_token = self.model.config.decoder_start_token_id\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokenized_input = self.tokenizer(text, return_tensors='pt', truncation=True)\n",
    "        return tokenized_input[\"input_ids\"]\n",
    "\n",
    "    def detokenize(self, token_ids):\n",
    "        return self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "\n",
    "    def forward(self, src_sequence, tgt_sequence):\n",
    "        \"\"\"\n",
    "        src_sequence: torch.LongTensor of shape (batch_size, seq_len) la séquence des ids des mots de la phrase source\n",
    "        tgt_sequence: torch.LongTensor of shape (batch_size, seq_len) la séquence des ids des mots de la phrase actuellement prédite\n",
    "\n",
    "        return: torch.FloatTensor of shape (batch_size, seq_len, vocab_size) la probabilité de chaque mot possible pour chaque token de la phrase actuellement prédite\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            encoder_states = self.model.get_encoder()(src_sequence)\n",
    "            decoder_states = self.model.get_decoder()(tgt_sequence, encoder_hidden_states=encoder_states.last_hidden_state)\n",
    "            prediction = self.model.lm_head(decoder_states.last_hidden_state)\n",
    "            prediction = torch.softmax(prediction, dim=-1)\n",
    "        return prediction\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "transformer = MyTransformer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans `transformer`, vous avez 2 méthodes que vous pouvez utiliser pour l'inférence: `transformer.tokenize` et `transformer.detokenize` pour convertir les phrases en nombres et vice versa.\n",
    "\n",
    "Lorsque vous faites ```transformer.tokenize(\"I love oranges.\")```, vous obtenez ```tensor([[47, 1779, 12610, 9, 3, 0]])```,\n",
    "\n",
    "Et lorsque vous faites ```transformer.detokenize([47, 1779, 12610, 9, 3, 0])```, vous obtenez ```['I', 'love', 'orange', 's', '.', '</s>']```.\n",
    "\n",
    "`</s>` est le token `<end>`.\n",
    "\n",
    "On remarque que le tokenizer sépare le mot \"orange\" en deux tokens, \"orange\" et \"s\". C'est juste une façon de faire de la tokenisation, et il y a d'autres façons de faire comme du mot par mot ou du caractère par caractère. ChatGPT par exemple utilise du syllabe par syllabe.\n",
    "\n",
    "Pour utiliser le modèle pré entraîné, vous allez devoir faire `transformer(input_ids, decoder_input_ids)`, où `input_ids` est la séquence tokenisée de la phrase source et `decoder_input_ids` est la séquence tokenisée de la phrase actuellement prédite. `input_ids` et `decoder_input_ids` doivent être des `torch.tensor` de shape `(batch_size, seq_len)` et de type `long`.\n",
    "\n",
    "Par exemple `transformer.tokenize(\"I love oranges.\")` est un valide `input_ids` car il est de shape `(1, 6)` et de type `long`.\n",
    "\n",
    "`transformer(input_ids, decoder_input_ids)` va retourner un `torch.tensor` de shape `(batch_size, seq_len, vocab_size)` qui correspond aux vecteurs de probabilité de chaque token de la phrase actuellement prédite. `vocab_size` est le nombre de mots dans le vocabulaire du modèle. Donc pour obtenir le mot suivant, vous allez devoir trouver l'élément avec la plus grande probabilité dans le dernier vecteur de probabilité de `transformer(input_ids, decoder_input_ids)`.\n",
    "\n",
    "`transformer.start_token` est l'id du token `<start>`. Vous allez devoir l'utiliser pour initialiser `decoder_input_ids` à la première étape de la boucle d'inférence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(transformer.tokenize(\"I love oranges.\"))\n",
    "print(transformer.detokenize([47, 1779, 12610, 9, 3, 0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Par exemple, si vous voulez prédire le premier mot de la phrase \"I love oranges.\", vous allez devoir faire:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = transformer.tokenize(\"I love oranges.\")\n",
    "decoder_input_ids = torch.tensor([[transformer.start_token]], dtype=torch.long)\n",
    "\n",
    "prediction = transformer(input_ids, decoder_input_ids)\n",
    "print(prediction.shape)\n",
    "\n",
    "id_next_word = torch.argmax(prediction[0, -1]).item()\n",
    "print(id_next_word)\n",
    "\n",
    "next_word = transformer.detokenize([id_next_word])\n",
    "print(next_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A vous de jouer\n",
    "Maintenant que vous savez comment utiliser le modèle, vous allez devoir faire la boucle d'inférence pour traduire la phrase \"I love oranges.\" en français en utilisant le modèle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalement, vous devriez obtenir \"J'adore les oranges\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entraînement\n",
    "Avant de regarder l'architecture d'un transfo, on va regarder comment on entraîne un transformer. La phase d'entraînement est un peu différente de la phase d'inférence.\n",
    "En fait, pendant l'entraînement, au lieu de prédire un token à la fois, le transformer va prédire tous les tokens de la target sequence (la phrase qu'on doit prédire) en même temps.\n",
    "\n",
    "Par exemple, reprenons l'exemple \"I love oranges.\". La phrase qu'on veut obtenir est \"J'aime les oranges.\". Au lieu de prédire un mot à la fois, le transformer va prédire `[J', aime, les, oranges, ., <end>]` (enfin, il va essayer).\n",
    "\n",
    "Plus précicément, on va donner au transformer les deux phrases `[I, love, oranges, .]` et `[<start>, J', aime, les, oranges, .]` en `input_ids` et `decoder_input_ids` respectivement, et on veut qu'il:\n",
    "    - prédise `J'` à partir de `[`<start>`]`\n",
    "    - prédise `aime` à partir de `[`<start>`, J']`\n",
    "    - prédise `les` à partir de `[`<start>`, J', aime]`\n",
    "    - prédise `oranges` à partir de `[`<start>`, J', aime, les]`\n",
    "    - prédise `.` à partir de `[`<start>`, J', aime, les, oranges]`\n",
    "    - prédise `<end>` à partir de `[`<start>`, J', aime, les, oranges, .]`\n",
    "\n",
    "Et le transformer va faire tout ça parallellement, va prédire directement tous les tokens `[J', aime, les, oranges, ., <end>]` en même temps.\n",
    "\n",
    "De plus, comme on donne `[<start>, J', aime, les, oranges, .]` à notre transformer, il faudra faire en sorte que le transformer ne puisse pas \"tricher\" en regardant directement les futurs tokens pour prédire le token actuel. On discutera de ça plus tard dans l'architecture du transformer.\n",
    "\n",
    "\n",
    "![training phase](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/prediction.png)\n",
    "\n",
    "Bref, donc notre target sequence (ou labels si vous voulez) est `[J', aime, les, oranges, ., <end>]`. Et on va devoir calculer la loss entre la prédiction du transformer et la target sequence.\n",
    "\n",
    "Voici un schéma qui illustre l'entraînement du transfo:\n",
    "\n",
    "![detailed training](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/prediction_6vectors.png)\n",
    "\n",
    "Remarque que comme la phrase actuellement prédite (ou `decoder_input_ids`, ou `inp_tgt` dans le schéma) contient 6 tokens, alors la prédiction du transformer (ou `out_tgt` dans le schéma) va contenir 6 vecteurs de probabilité aussi, et que notre target sequence (ou `tgt` dans le schéma) contient 6 tokens également.\n",
    "\n",
    "On va calculer la cross entropy loss entre chaque vecteur de probabilité de la prédiction du transformer et chaque token de la target sequence. Donc on va avoir 6 loss, et on va faire la moyenne de ces 6 loss pour avoir la loss finale.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: calculer la cross entropy loss du modèle `transformer` pour la phrase \"I love oranges.\" en français (\"J'aime les oranges.\")\n",
    "Sachant que `[234, 6, 17711, 16, 12610, 9, 3, 0]` correspond à la tokenisation de la séquence `[J, ', adore, les, orange, s, ., <end>]` en français.\n",
    "Calculer la moyenne des 6 cross entropy loss entre la prédiction du transformer et la target sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "target_ids = [234, 6, 17711, 16, 12610, 9, 3, 0]\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalement, vous devriez obtenir une loss de 10.526789665222168. Si vous obtenez une loss de 10.6... (sur colab) par exemple ce n'est probablement pas normal, il faudra vérifier votre code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## L'architecture du transformer\n",
    "Maintenant que vous avez compris les entrées et les sorties du transformer, on va regarder l'architecture du transformer.\n",
    "\n",
    "![transformer architecture](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/architecture.png)\n",
    "\n",
    "Dans le transformer, deux séquences sont données en entrée: Inputs (`input_ids` dans notre cas) et Outputs (shifted right) (`decoder_input_ids` dans notre cas).\n",
    "\n",
    "![input and output shifted right](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/inputs_archi.png)\n",
    "\n",
    "Dans le cas de la traduction, Inputs est la phrase à traduire, et Outputs (shifted right) est la phrase actuellement prédite.\n",
    "\n",
    "Par exemple, pour notre cas Inputs est `[I, love, oranges, .]` et Outputs (shifted right) est:\n",
    "    - `[<start>, J', aime, les, oranges, .]` si on est dans la phase d'entraînement\n",
    "    - `[<start>]` si on est au début de la phase d'inférence, et `[<start>, J']` si on a déjà prédit `J'` et qu'on veut prédire `aime` maintenant, etc.\n",
    "\n",
    "### Inputs embedding\n",
    "\n",
    "![inputs](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/input_emb_box.png)\n",
    "\n",
    "La couche inputs embedding sert juste à transformer les tokens en vecteurs de dimension `d_model`. Pour du traitement de texte, on utilise la plupart de temps la couche `nn.Embedding` que vous avez vu dans le TP sur l'introduction à NLP.\n",
    "\n",
    "Exemple:\n",
    "![embedding](https://raw.githubusercontent.com/uyitroa/draft-transfo-wiki/main/exembedding.png)\n",
    "Ici, le token 10 est transformé en vecteur `[0.2, -0.5, ..., 1.2, -0.7]`.\n",
    "\n",
    "Les valeurs de ces vecteurs sont apprises pendant l'entraînement à l'aide de la backpropagation. Durant l'entraînement, le transformer va ajuster ces valeurs pour que les vecteurs représentent les features les plus utiles et informatives des tokens.\n",
    "\n",
    "Intuitivement, on pourrait imaginer que les mots synonymes auront des vecteurs similaires, et les mots antonymes auront des vecteurs opposés."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

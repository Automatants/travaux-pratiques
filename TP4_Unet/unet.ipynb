{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction à Unets\n",
    "L'architecture Unet est un réseau de neurones initialement développé pour la segmentation d'images biomédicales à l'Université de Freiburg, en Allemagne.\n",
    "L'Unet est une sorte d'autoencodeur mais avec des connections résiduelles entre l'encodeur et le décodeur afin d'éviter la disparition du gradient et de connecter les informations extraites par l'encodeur au décodeur plus facilement. Ces connections permettent au réseasu de capturer et de propager efficacement les informations de contexte.\n",
    "\n",
    "![Unet architecture](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/TP4_Unet/u-net-architecture.png)\n",
    "\n",
    "\n",
    "L'Unet est composée d'une structure encodeur-décodeur avec des connexions de saut (skip connections), lui permettant de capturer à la fois des features de haut et bas niveau.\n",
    "\n",
    "L'architecture Unet en détail:\n",
    "\n",
    "Encodeur : L'encodeur est responsable de l'extraction des features de l'image en entrée. Il consiste de plusieurs blocs de couche de convolution et un maxpooling entre chaque bloc pour réduire la taille de l'image.\n",
    "\n",
    "Bridge: Le bridge sert à connecter les features extraits par l'encodeur à chaque niveau à l'entrée du décodeur, en concatenant la sortie de chaque bloc de l'encodeur à la sortie du bloc correspondant du décodeur.\n",
    "\n",
    "Décodeur : Le décodeur s'occupe de la tâche essentielle de remapper les features de haut niveau extraites par l'encodeur en une image de sortie reconstruite. Il est composé également de plusieurs blocs, et utilise des couches de convolution transposées pour effectuer l'upsampling des features, rétablissant ainsi leurs dimensions originales. Chaque bloc du décodeur reçoit non seulement les données upsampled du bloc précédent, mais aussi les données de l'encodeur correspondant grâce aux connexions de saut. Ces données concaténées sont ensuite transmises à travers des couches de convolution pour affiner les features upsampled.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Debruitage des images\n",
    "\n",
    "On va essayer d'améliorer notre modèle de débruitage d'images qu'on a fait en TP3 en utilisant l'architecture UNet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importation des librairies et téléchargements de MNIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "X_train = train_dataset.data\n",
    "X_test = test_dataset.data\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercice 1: Normalisation des images et ajout de bruit\n",
    "Normaliser les images en [0, 1] et ajouter du bruit sur les images de train et de test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_normalized = ...\n",
    "X_test_normalized = ...\n",
    "...\n",
    "X_train_noise = ...\n",
    "X_test_noise = ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercice 2: Visualisation des données bruitées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercice 3: Création du modèle\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encodeur\n",
    "Voici le schéma de l'encodeur qu'on va créer (avec des ReLU après chaque couche de convolution):\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Layer (type)                   Output Shape              \n",
    "# ================================================================\n",
    "# input_1               (1, 28, 28)\n",
    "# ________________________________________________________________\n",
    "# conv2d_1              (32, 28, 28)\n",
    "# ________________________________________________________________\n",
    "# maxpooling2d_1        (32, 14, 14)\n",
    "# ________________________________________________________________\n",
    "# conv2d_2              (64, 14, 14)\n",
    "# ________________________________________________________________\n",
    "# maxpooling2d_2        (64, 7, 7)\n",
    "# ________________________________________________________________\n",
    "# conv2d_3 (Conv2D)     (128, 7, 7)\n",
    "# ________________________________________________________________\n",
    "# maxpooling2d_3        (128, 3, 3)\n",
    "# ________________________________________________________________\n",
    "# conv2d_4              (256, 3, 3)\n",
    "# ________________________________________________________________\n",
    "# maxpooling2d_4        (256, 1, 1)\n",
    "# ________________________________________________________________\n",
    "# conv2d_5              (256, 1, 1)\n",
    "# ________________________________________________________________\n",
    "# outputs               [(32, 28, 28), (64, 14, 14), (128, 7, 7), (256, 3, 3), (256, 1, 1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il faut retourner non seulement la sortie de conv2d_5 mais aussi les sorties de conv2d_1, conv2d_2, conv2d_3 et conv2d_4 pour les utiliser dans le décodeur.\n",
    "Vous pouvez par exemple mettre les sorties dans une liste et les retourner."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t...\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t...\n",
    "\t\treturn [...]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Torchinfo pour vérifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 5\n",
    "encoder = Encoder()\n",
    "summary(encoder, input_size=(batch_size, 1, 28, 28))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Décodeur\n",
    "Pour concatener, regarder la fonction torch.cat: https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "\n",
    "Voici le schéma du décodeur qu'on va créer (avec des ReLU après chaque couche de convolution):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Layer (type)                   Output Shape\n",
    "# ================================================================\n",
    "# input_1               [(32, 28, 28), (64, 14, 14), (128, 7, 7), (256, 3, 3), (256, 1, 1)]\n",
    "# ________________________________________________________________\n",
    "# conv2d_transpose_1    (256, 3, 3)\n",
    "# ________________________________________________________________\n",
    "# concatenate_1         (512, 3, 3)\n",
    "# ________________________________________________________________\n",
    "# conv2d_6              (256, 3, 3)\n",
    "# ________________________________________________________________\n",
    "# conv2d_transpose_2    (128, 7, 7)\n",
    "# ________________________________________________________________\n",
    "# concatenate_2         (256, 7, 7)\n",
    "# ________________________________________________________________\n",
    "# conv2d_7              (128, 7, 7)\n",
    "# ________________________________________________________________\n",
    "# conv2d_transpose_3    (64, 14, 14)\n",
    "# ________________________________________________________________\n",
    "# concatenate_3         (128, 14, 14)\n",
    "# ________________________________________________________________\n",
    "# conv2d_8              (64, 14, 14)\n",
    "# ________________________________________________________________\n",
    "# conv2d_transpose_4    (32, 28, 28)\n",
    "# ________________________________________________________________\n",
    "# concatenate_4         (64, 28, 28)\n",
    "# ________________________________________________________________\n",
    "# conv2d_9              (1, 28, 28)\n",
    "# ________________________________________________________________\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t...\n",
    "\n",
    "\tdef forward(self, *encoder_outputs: List[torch.Tensor]):\n",
    "\t\t...\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Torchinfo pour vérifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "batch_size = 5\n",
    "decoder = Decoder()\n",
    "summary(decoder, input_size=[(batch_size, 32, 28, 28), (batch_size, 64, 14, 14), (batch_size, 128, 7, 7), (batch_size, 256, 3, 3), (batch_size, 256, 1, 1)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder()\n",
    "\t\tself.decoder = Decoder()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\touts = self.encoder(x)\n",
    "\t\tout = self.decoder(*outs)\n",
    "\t\treturn out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "unet = Unet()\n",
    "summary(decoder, input_size=(batch_size, 1, 28, 28))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercice 4: Entraînement du modèle (sur GPU) et visualisation des résultats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenge: Denoising Dirty Documents/Remove noise from printed text\n",
    "\n",
    "![Dirty Documents](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/TP3_Autoencodeur/denoisingdocument.JPG)\n",
    "\n",
    "Lien vers le challenge: https://sharing.cs-campus.fr/compete/89"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

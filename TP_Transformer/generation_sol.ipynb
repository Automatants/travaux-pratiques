{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Prérequis: Avoir fait le tp Transformer.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Génération de texte\n",
    "Félicitation d'avoir fini le tp sur les transformers (ou pas). Ici, on va s'amuser à générer du texte avec un transformer (ou presque)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Idée\n",
    "Je ne sais pas si vous avez remarqué, mais pour de la génération de texte, il n'y pas de phrase à encoder, contrairement à la traduction de texte où la phrase à encoder est la phrase à traduire.\n",
    "\n",
    "On va donc utiliser juste la partie decodeur du transformer. Mais si vous vous souvenez, il y a la couche Cross Attention dans la partie decodeur, qui n'est pas nécessaire ici car on n'a pas de phrase à encoder. On va donc utiliser un decoduer sans la Cross Attention.\n",
    "\n",
    "![decoderonly](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/decoderonly.png)\n",
    "\n",
    "Bref, l'idée est la suivante: on va entraîner notre modèle à compléter des phrases.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "Entrée: [\"\\<start>\", \"Je\", \"suis\", \"un\", \"chat\", \"<end>\"]\n",
    "\n",
    "Sortie: [\"Je\", \"suis\", \"un\", \"chat\", \"\\<end>\"]\n",
    "\n",
    "Le modèle va alors:\n",
    "- Prédire \"Je\" à partir de \"\\<start>\"\n",
    "- Prédire \"suis\" à partir de \"\\<start> Je\"\n",
    "- Prédire \"un\" à partir de \"\\<start> Je suis\"\n",
    "- etc...\n",
    "\n",
    "Puis, pendant l'inference, on va juste lui donner \"\\<start>\" et il va compléter la phrase jusqu'à \"\\<end>\".\n",
    "\n",
    "Si vous vous rappelez, la sortie de notre modèle est des vecteurs de probabilité et dans le tp précédent, on a utilisé l'argmax de ces vecteurs de probabilité pour avoir le mot prédit.\n",
    "\n",
    "Mais si ici on prend l'argmax du vecteur de probabilité à chaque fois, on va avoir juste avoir la même phrase à chaque fois qu'on génère du texte.\n",
    "\n",
    "Donc on va utiliser un autre moyen pour générer du texte: le sampling.\n",
    "\n",
    "Le sampling consiste à prendre un mot en fonction de sa probabilité. Par exemple, si le mot \"chat\" a une probabilité de 0.8, on va le prendre 80% du temps.\n",
    "\n",
    "Donc avec les vecteurs de probabilité que notre modèle nous donne, on va prendre un mot en fonction de sa probabilité.\n",
    "\n",
    "Avant de sampler, on va diviser les vecteurs (avant le softmax) par un facteur qu'on appelle la température. La température est un hyperparamètre qui va déterminer la diversité du texte généré. Plus la température est grande, plus le texte généré sera diversifié. Plus la température est petite, plus le texte généré sera similaire à l'entrée, mais plus robuste à l'erreur, car avec une température trop grande, les probabilités des mots vont être trop proches et on risque d'avoir des mots qui n'ont pas de sens.\n",
    "\n",
    "Remarque: C'est exactement ce que fait ChatGPT. Il est aussi un decoder-only transformer mais il a 175 milliards de paramètres, un peu plus gros que le modèle qu'on va faire ici."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "On va utiliser le dataset WikiText-2.\n",
    "\n",
    "Il faut tout d'abord installer torchtext et portalocker."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torchtext portalocker"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import WikiText2\n",
    "import torch\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = WikiText2(split='train')\n",
    "\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\", \"<pad>\", \"<start>\", \"<end>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train = []\n",
    "max_len = 500\n",
    "for sentence in train_iter:\n",
    "    x_train.append(torch.tensor(vocab([\"<start>\"] + tokenizer(sentence)[:500] + [\"<end>\"])))\n",
    "\n",
    "x_train = torch.nn.utils.rnn.pad_sequence(x_train, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "pad_train = (x_train != vocab[\"<pad>\"]).bool()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataloader pour le training containing x_train and pad_train\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, pad_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = torch.nn.Linear(d_model, d_model)\n",
    "        self.wk = torch.nn.Linear(d_model, d_model)\n",
    "        self.wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, key_padding_mask=None, attn_mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float))\n",
    "        if attn_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        if key_padding_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2) == 0, -1e9)\n",
    "        attn_scores = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        attn_output = torch.matmul(attn_scores, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        attn_output = self.fc(attn_output)\n",
    "\n",
    "        return attn_output, attn_scores\n",
    "\n",
    "\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward=2048, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.multihead_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.linear1 = torch.nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = torch.nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm3 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + tgt2\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        tgt2 = self.linear2(torch.nn.functional.relu(self.linear1(tgt)))\n",
    "        tgt = tgt + tgt2\n",
    "        tgt = self.norm3(tgt)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, num_layers, norm=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([DecoderLayer(d_model, num_heads, dim_feedforward) for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "\n",
    "        self.max_len = max_len+1\n",
    "        self.register_buffer('tgt_mask', torch.tril(torch.ones(self.max_len, self.max_len)))\n",
    "\n",
    "    def forward(self, tgt, tgt_key_padding_mask=None):\n",
    "        sen_len = tgt.size(1)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, tgt_mask=self.tgt_mask[:sen_len, :sen_len], tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            tgt = self.norm(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = torch.nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, dim_feedforward, num_layers)\n",
    "        self.out = torch.nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, tgt, tgt_key_padding_mask=None):\n",
    "        tgt = self.encoder(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        tgt = self.decoder(tgt, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        output = self.out(tgt)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(len(vocab), d_model=128, num_heads=4, num_layers=4, dim_feedforward=1024).to(device)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# training\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_dataloader))\n",
    "    for idx, batch in pbar:\n",
    "        tgt, tgt_key_padding_mask = batch\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_key_padding_mask = tgt_key_padding_mask.to(device)\n",
    "\n",
    "        ground_truth = tgt[:, 1:]\n",
    "        tgt = tgt[:, :-1]\n",
    "\n",
    "        output = model(tgt, tgt_key_padding_mask=tgt_key_padding_mask[:, :-1])\n",
    "\n",
    "        loss_val = loss(output.transpose(1, 2), ground_truth)\n",
    "        loss_val = loss_val.masked_select(ground_truth != vocab['<pad>']).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'epoch {epoch} loss {loss_val.item():.4f}')\n",
    "        break\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "\n",
    "            print(\"Input: \", ' '.join(vocab.lookup_tokens(tgt[0].tolist())))\n",
    "            print(\"Output: \", ' '.join(vocab.lookup_tokens(ground_truth[0].tolist())))\n",
    "\n",
    "            # evaluation\n",
    "            model.eval()\n",
    "            cur_sentence = [vocab['<start>']]\n",
    "            temperature = 0.8\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i in range(100):\n",
    "                    cur_input = torch.tensor(cur_sentence).unsqueeze(0).to(device)\n",
    "                    output = model(cur_input)\n",
    "                    output = output[:, -1, :] / temperature\n",
    "                    output = torch.nn.functional.softmax(output, dim=-1)\n",
    "                    output = torch.multinomial(output, num_samples=1).squeeze(0)\n",
    "                    cur_sentence.append(output.item())\n",
    "                    if output.item() == vocab['<end>']:\n",
    "                        break\n",
    "\n",
    "            print(' '.join(vocab.lookup_tokens(cur_sentence)))\n",
    "\n",
    "            model.train()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "cur_sentence = [vocab['<start>']]\n",
    "temperature = 0.8\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        cur_input = torch.tensor(cur_sentence).unsqueeze(0).to(device)\n",
    "        output = model(cur_input)\n",
    "        temperature_tensor = torch.full_like(output, temperature)\n",
    "        # put 1000 to <unk> and <pad>\n",
    "        temperature_tensor[:, :, vocab['<unk>']] = 1000\n",
    "        output = output[:, -1, :] / temperature_tensor[:, -1, :]\n",
    "        print(output.shape)\n",
    "        output = torch.nn.functional.softmax(output, dim=-1)\n",
    "        output = torch.multinomial(output, num_samples=1).squeeze(0)\n",
    "        cur_sentence.append(output.item())\n",
    "        if output.item() == vocab['<end>']:\n",
    "            break\n",
    "\n",
    "print(' '.join(vocab.lookup_tokens(cur_sentence)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING (DQN) TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tutoriel montre comment utiliser `PyTorch` pour entraîner un agent Deep Q Learning (DQN) sur `CartPole-v1` de `Gymnasium`.\n",
    "\n",
    "**Tâche** : L'agent doit décider entre deux `actions` : déplacer le chariot vers la `gauche` ou vers la `droite` - afin que le poteau qui lui est attaché reste en équilibre. Vous pouvez trouver plus d'informations sur l'environnement et sur d'autres environnements plus difficiles sur le site web de Gymnasium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](../image-hosting/TP8_intro_RL/cartpole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'agent observe l'état actuel de l'environnement et choisit une action, l'environnement passe à un nouvel état et renvoie également une `récompense` qui indique les conséquences de l'action. Dans cette tâche, les récompenses sont de `+1` pour chaque pas de temps incrémental, et l'environnement se termine si le poteau tombe trop loin ou si le chariot se déplace de plus de 2,4 unités par rapport au centre. Cela signifie que les scénarios qui se déroulent mieux durent plus longtemps et accumulent une récompense plus importante.\n",
    "\n",
    "La tâche CartPole est conçue de manière à ce que les entrées pour l'agent soient `4 valeurs réelles` représentant l'`état` de l'environnement (position, vitesse, etc.). Nous prenons ces 4 entrées sans mise à l'échelle et les passons à travers un petit réseau entièrement connecté avec `2 sorties`, une pour chaque action. Le réseau est entraîné à prédire la valeur attendue pour chaque action, en fonction de l'état d'entrée. L'action ayant la valeur attendue la plus élevée est ensuite choisie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer les packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import keyboard\n",
    "import random\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement nous est donné par `gym` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez essayer vous même cet environnement. Utilisez les touches directionnelles pour contrôler le cart. Par défault la cart va à gauche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "games = 20 # nombre de parties à jouer\n",
    "\n",
    "env.reset()\n",
    "\n",
    "action = 0\n",
    "n = 0\n",
    "\n",
    "img = plt.imshow(env.render()) # only call this once\n",
    "while n < games:\n",
    "   img.set_data(env.render()) # just update the data\n",
    "   display.display(plt.gcf())\n",
    "   display.clear_output(wait=True)\n",
    "   if keyboard.is_pressed('left'):\n",
    "      action = 0\n",
    "   elif keyboard.is_pressed('right'):\n",
    "      action = 1\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   if terminated or truncated:\n",
    "      env.reset()\n",
    "      action = 0\n",
    "      n += 1\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "**Transition** : Une transition est un tuple `(state, action, next_state, reward)`.\n",
    "\n",
    "Nous utiliserons une mémoire de relecture d'expérience pour entraîner notre DQN. Elle stocke les transitions que l'agent observe, ce qui nous permet de réutiliser ces données ultérieurement. En les échantillonnant de manière aléatoire, les transitions qui composent un lot sont décorrélées. Il a été démontré que cela stabilise considérablement et améliore la procédure d'entraînement du DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer une `class` pour reprensenter cette mémoire. Cette `class` possede deux méthodes:\n",
    "- push : ajoute une transition à la mémoire. Une transition est représentée par un dictionnaire : `{state : value, action : value, next_state : value, reward: value}`.\n",
    "- sample : échantillonne `batch_size` transition. Un échantillon est un dictionnaire de la forme : `{state : list_value, action : list_value, next_state : list_value, reward: list_value}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour implémenter `sample` utiliser les fonctions `zip` et `random.sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        pass\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme DQN\n",
    "\n",
    "Notre environnement est déterministe, donc toutes les équations présentées ici sont également formulées de manière déterministe pour simplifier les choses. Dans la littérature sur l'apprentissage par renforcement, elles contiendraient également des attentes sur les transitions stochastiques dans l'environnement.\n",
    "\n",
    "Notre objectif sera de former une politique qui tente de maximiser la récompense cumulative actualisée, $R_{t_0} = \\sum_{t=t_0}^\\infty \\gamma^{t - t_0} r_t$, où $R_{t_0}$ est également connu sous le nom de rendement. La réduction, $\\gamma$, doit être une constante entre $0$ et $1$ pour assurer que la somme converge. Une valeur plus faible de $\\gamma$ rend les récompenses du futur incertain moins importantes pour notre agent que celles du futur proche dont il peut être assez confiant. Cela encourage également les agents à collecter des récompenses plus proches dans le temps que des récompenses équivalentes qui sont temporellement éloignées dans le futur.\n",
    "\n",
    "L'idée principale derrière le $Q$-learning est que si nous avions une fonction $Q^* : \\text{État} \\times \\text{Action} \\rightarrow \\mathbb{R}$ qui pourrait nous dire quelle serait notre récompense si nous prenions une action dans un état donné, alors nous pourrions facilement construire une politique qui maximise nos récompenses :\n",
    "\n",
    "$\\pi^*(s) = \\arg\\max_a Q^*(s, a)$\n",
    "\n",
    "Cependant, nous ne savons pas tout sur le monde, donc nous n'avons pas accès à $Q^*$. Mais, puisque les réseaux neuronaux sont des approximateurs de fonctions universels, nous pouvons simplement en créer un et le former pour ressembler à $Q^*$.\n",
    "\n",
    "Pour notre règle de mise à jour de formation, nous utiliserons le fait que chaque fonction $Q$ pour une certaine politique obéit à l'équation de Bellman :\n",
    "\n",
    "$Q^\\pi(s, a) = r + \\gamma Q^\\pi(s', \\pi(s'))$\n",
    "\n",
    "La différence entre les deux côtés de l'égalité est connue sous le nom d'erreur de différence temporelle, $\\delta$ :\n",
    "\n",
    "$\\delta = Q(s, a) - \\left(r + \\gamma \\max_{a'} Q(s', a')\\right)$\n",
    "\n",
    "Pour minimiser cette erreur, nous utiliserons la perte de Huber. La perte de Huber se comporte comme l'erreur quadratique moyenne lorsque l'erreur est petite, mais comme l'erreur absolue moyenne lorsque l'erreur est grande - cela la rend plus robuste aux valeurs aberrantes lorsque les estimations de $Q$ sont très bruitées. Nous calculons cela sur un lot de transitions, $\\mathcal{B}$, échantillonné depuis la mémoire de répétition :\n",
    "\n",
    "$L = \\frac{1}{|\\mathcal{B}|} \\sum_{(s, a, s', r) \\in \\mathcal{B}} \\mathcal{L}(\\delta)$\n",
    "\n",
    "où\n",
    "\n",
    "$\\mathcal{L}(\\delta) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{2} \\delta^2 & \\text{pour } |\\delta| \\leq 1, \\\\\n",
    "| \\delta | - \\frac{1}{2} & \\text{sinon}.\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par définir notre modèle. Notre modèle à pour but d'approximer la fonction $Q$. Le nombre d'entrées est donc égale au nombre d'observation et le nombre de sorties est égale au nombre d'action. Avec ce formalisme, si `dqn` est notre réseau de neuronnes et `state` l'état de l'environnement alors :\n",
    "- `dqn(state)` est un `tensor` contenant les `Q-values` de chacune des deux actions.\n",
    "- `argmax dqn(state)` est l'action qui doit être choisie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'observations : 4\n",
      "nombre d'actions : 2\n"
     ]
    }
   ],
   "source": [
    "n_observations = len(env.reset()[0])\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"nombre d'observations : {n_observations}\")\n",
    "print(f\"nombre d'actions : {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        torch.nn.Module.__init__(self)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici deux notions importantes à comprendre avant de continuer ce tp.\n",
    "\n",
    "**Epsilon-Greedy :**\n",
    "\n",
    "L'epsilon-greedy est une stratégie d'exploration utilisée dans l'apprentissage par renforcement. Elle permet à un agent de choisir entre explorer de nouvelles actions (exploration) et choisir l'action qui semble actuellement la meilleure en fonction de ses connaissances actuelles (exploitation). \n",
    "\n",
    "- Lorsque l'agent utilise l'exploration, il sélectionne une action au hasard avec une probabilité epsilon (ε). Cela permet à l'agent de découvrir de nouvelles actions et de collecter des informations sur leur efficacité.\n",
    "- Lorsque l'agent utilise l'exploitation, il choisit l'action qui a la meilleure estimation de valeur en fonction de ses connaissances actuelles.\n",
    "\n",
    "L'idée derrière l'epsilon-greedy est de trouver un équilibre entre l'exploration et l'exploitation. Pendant l'apprentissage initial, l'exploration est favorisée (ε est élevé) pour découvrir rapidement des stratégies potentiellement meilleures. Au fil du temps, l'exploitation devient plus prédominante (ε diminue) à mesure que l'agent acquiert une connaissance plus solide de l'environnement.\n",
    "\n",
    "**Target Net :**\n",
    "\n",
    "Le \"target net\" (ou réseau cible) est un composant essentiel dans les algorithmes d'apprentissage profond par renforcement, tels que DQN (Deep Q-Network). Il est utilisé pour stabiliser et améliorer l'apprentissage de l'agent.\n",
    "\n",
    "Dans un DQN, il existe deux réseaux de neurones principaux : le \"policy net\" (ou réseau de politique) et le \"target net.\" Le réseau de politique est utilisé pour estimer la meilleure action à prendre dans un état donné. Le réseau cible, quant à lui, est utilisé pour estimer la valeur future des états.\n",
    "\n",
    "Le réseau cible est mis à jour moins fréquemment que le réseau de politique. Cette mise à jour lente permet de stabiliser l'apprentissage en évitant les oscillations potentielles des valeurs Q (fonction d'évaluation) lors de la mise à jour fréquente du réseau de politique.\n",
    "\n",
    "En pratique, le réseau cible est généralement mis à jour périodiquement en copiant les poids du réseau de politique vers le réseau cible. Cela permet de maintenir une estimation plus stable des valeurs Q à long terme, ce qui améliore la convergence de l'algorithme d'apprentissage par renforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons ici les hyperparamètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la fonction `select_action` pour qu'elle renvoie l'action choisie par votre `model` dans l'état `state`. Voici un exemple d'implémentation possible :\n",
    "- Si `argmax dqn(state) == 0`, aller à gauche.\n",
    "- Si `argmax dqn(state) == 1`, aller à droite.\n",
    "\n",
    "N'oubliez pas d'implémenter l'algorithme `epsilon-greedy` en choisissant une action au hasard avec une probabilité de $\\varepsilon_{end} + (\\varepsilon_{start} - \\varepsilon_{end}) * e ^ {-\\varepsilon_{decay} \\cdot step}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "def select_action(state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maitenant coder la boucle d'entrainement de nôtre modèle. La partie compliquée est le calcule de la `loss`\n",
    "\n",
    "$\\delta = Q(s, a) - \\left(r + \\gamma \\max_{a'} Q(s', a')\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

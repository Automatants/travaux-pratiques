{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Prérequis: Avoir fait le tp Transformer.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Génération de texte\n",
    "Félicitation d'avoir fini le tp sur les transformers (ou pas). Ici, on va s'amuser à générer des histoires avec un transformer (ou presque).\n",
    "\n",
    "N'oubliez pas de mettre le runtime en GPU si vous êtes sur colab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Idée\n",
    "Je ne sais pas si vous avez remarqué, mais pour de la génération de texte, il n'y pas de phrase à encoder, contrairement à la traduction de texte où la phrase à encoder est la phrase à traduire.\n",
    "\n",
    "On va donc utiliser juste la partie decodeur du transformer. Mais si vous vous souvenez, il y a la couche Cross Attention dans la partie decodeur, qui n'est pas nécessaire ici car on n'a pas de phrase à encoder. On va donc utiliser un decoduer sans la Cross Attention.\n",
    "\n",
    "![decoderonly](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/decoderonly.png)\n",
    "\n",
    "Bref, l'idée est la suivante: on va entraîner notre modèle à compléter des phrases.\n",
    "\n",
    "Exemple:\n",
    "\n",
    "Entrée: [\"\\<start>\", \"Je\", \"suis\", \"un\", \"chat\", \"<end>\"]\n",
    "\n",
    "Sortie: [\"Je\", \"suis\", \"un\", \"chat\", \"\\<end>\"]\n",
    "\n",
    "Le modèle va alors:\n",
    "- Prédire \"Je\" à partir de \"\\<start>\"\n",
    "- Prédire \"suis\" à partir de \"\\<start> Je\"\n",
    "- Prédire \"un\" à partir de \"\\<start> Je suis\"\n",
    "- etc...\n",
    "\n",
    "Puis, pendant l'inference, on va juste lui donner \"\\<start>\" et il va compléter la phrase jusqu'à \"\\<end>\".\n",
    "\n",
    "Si vous vous rappelez, la sortie de notre modèle est des vecteurs de probabilité et dans le tp précédent, on a utilisé l'argmax de ces vecteurs de probabilité pour avoir le mot prédit.\n",
    "\n",
    "Mais si ici on prend l'argmax du vecteur de probabilité à chaque fois, on va avoir juste avoir la même phrase à chaque fois qu'on génère du texte.\n",
    "\n",
    "Donc on va utiliser un autre moyen pour générer du texte: le sampling.\n",
    "\n",
    "Le sampling consiste à prendre un mot en fonction de sa probabilité. Par exemple, si le mot \"chat\" a une probabilité de 0.8, on va le prendre 80% du temps.\n",
    "\n",
    "Donc avec les vecteurs de probabilité que notre modèle nous donne, on va prendre un mot en fonction de sa probabilité.\n",
    "\n",
    "Avant de sampler, on va diviser les vecteurs (avant le softmax) par un facteur qu'on appelle la température. La température est un hyperparamètre qui va déterminer la diversité du texte généré. Plus la température est grande, plus le texte généré sera diversifié. Plus la température est petite, plus le texte généré sera similaire à l'entrée, mais plus robuste à l'erreur, car avec une température trop grande, les probabilités des mots vont être trop proches et on risque d'avoir des mots qui n'ont pas de sens.\n",
    "\n",
    "Remarque: C'est exactement ce que fait ChatGPT. Il est aussi un decoder-only transformer mais il a 175 milliards de paramètres, un peu plus gros que le modèle qu'on va faire ici."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Préparation des données\n",
    "On va utiliser le dataset WritingPrompts. C'est un dataset qui contient les prompts et les histoires associées.\n",
    "\n",
    "Pour l'instant, on va utiliser que les histoires pour l'entraînement."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/colab_train.txt\"\n",
    "r = requests.get(url)\n",
    "dataset = r.text\n",
    "\n",
    "# save file\n",
    "with open(\"colab_train.txt\", \"w\") as f:\n",
    "    f.write(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`dataset` est une string qui contient toutes les histoires. Chaque ligne correspond à une histoire.\n",
    "`<newline>` est un token qui correspond à un retour à la ligne.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset.split(\"\\n\")[0])  # On affiche la première histoire"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    "Ici, on va utiliser la tokenization Byte-Pair Encoding (BPE). C'est une tokenization qui va découper les mots en sous-mots. Par exemple, \"chat\" peut être découpé en \"cha\" et \"t\". C'est une tokenization qui est très utilisée dans les modèles de génération de texte, par exemple ChatGPT.\n",
    "La tokenization mot par mot est aussi possible, mais elle n'est pas efficace. Par exemple, pour notre modèle (avant l'entraînement), \"eat\" et \"eating\" sont deux mots différents. Donc si on utilise une tokenization mot par mot, on va avoir deux embeddings différents pour \"eat\" et \"eating\", alors qu'ils devraient être liés en quelque sorte.\n",
    "\n",
    "### Principes du BPE\n",
    "La première question qu'on se pose est alors: Comment découper les mots en sous-mots?\n",
    "\n",
    "On va découper les mots en sous-mots en fonction de la fréquence des sous-mots. Par exemple, si \"chat\" et \"chien\" sont deux mots très fréquents, alors \"ch\" va être un sous-mot très fréquent.\n",
    "\n",
    "On part d'un vocabulaire de base constitué de caractères individuels.\n",
    "Au début, on peut considérer chaque mot comme une séquence de caractères: \"chat\" = \"c\", \"h\", \"a\", \"t\".\n",
    "\n",
    "Ensuite, on compte la fréquence de chaque paire de caractères (ou bigrammes) adjacents dans le corpus. La paire de caractères la plus fréquente est ensuite fusionnée pour former un nouveau token du vocabulaire. Par exemple, si \"ch\" est la paire de caractères la plus fréquente, on va fusionner \"c\" et \"h\" pour former un nouveau token \"ch\".\n",
    "\n",
    "On répète ce processus plusieurs fois, en fusionnant à chaque fois les paires de tokens les plus fréquentes pour former de nouveaux tokens. À chaque itération, le vocabulaire augmente avec l'ajout de nouveaux sous-mots.\n",
    "\n",
    "Remarque alors la tokenization BPE dépend du dataset.\n",
    "Il faut donc \"entraîner\" la tokenization BPE sur notre dataset.\n",
    "\n",
    "### Implémentation\n",
    "\n",
    "On ne va pas l'implémenter car ce n'est pas le but de ce TP, on va donc utiliser la librarie sentencepiece qui contient déjà le BPE.\n",
    "Il faut installer sentencepiece avec `pip install sentencepiece`.\n",
    "Si vous êtes sur colab: `!pip install sentencepiece`\n",
    "\n",
    "#### Entraînement\n",
    "On va entraîner la tokenization BPE sur notre dataset et on choisit d'avoir 2500 tokens au maximum.\n",
    "Donc notre tokenization BPE contiendra les 2500 sous-mots les plus fréquents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "input_file = 'colab_train.txt'\n",
    "max_num_words = 2500\n",
    "model_type = 'bpe'\n",
    "model_prefix = 'sentencepiece'\n",
    "pad_id = 0\n",
    "unk_id = 1\n",
    "start_id = 2\n",
    "end_id = 3\n",
    "\n",
    "sentencepiece_params = ' '.join([\n",
    "\t'--input={}'.format(input_file),\n",
    "\t'--model_type={}'.format(model_type),\n",
    "\t'--model_prefix={}'.format(model_prefix),\n",
    "\t'--vocab_size={}'.format(max_num_words),\n",
    "\t'--pad_id={}'.format(pad_id),\n",
    "\t'--unk_id={}'.format(unk_id),\n",
    "\t'--bos_id={}'.format(start_id),\n",
    "\t'--eos_id={}'.format(end_id)\n",
    "])\n",
    "\n",
    "print(sentencepiece_params)\n",
    "SentencePieceTrainer.train(sentencepiece_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Utilisation\n",
    "Tout d'abord, on va load le modèle qu'on vient d'entraîner."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "sp = SentencePieceProcessor()\n",
    "sp.load(\"{}.model\".format(model_prefix))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour tokeniser une phrase, on utilise la fonction `encode_as_pieces`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sp.encode_as_pieces(\"Je suis un chat\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour décoder une liste de tokens, on utilise la fonction `decode_pieces`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenised = sp.encode_as_pieces(\"Je suis un chat\")\n",
    "sp.decode_pieces(tokenised)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut aussi encoder une phrase en entier avec la fonction `encode_as_ids`. Ça évite d'utiliser vocab de `torchtext`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sp.encode_as_ids(\"Je suis un chat\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour décoder une liste d'ids, on utilise la fonction `decode_ids`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = sp.encode_as_ids(\"Je suis un chat\")\n",
    "sp.decode_ids(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour obtenir la taille du vocabulaire, on utilise la fonction `get_piece_size`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sp.get_piece_size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch-Lightning\n",
    "Pour ce TP, on va utiliser pytorch lightning pour nous simplifier la vie. On va utiliser la précision '16-mixed' afin de réduire la mémoire utilisée par le modèle et donc d'augmenter la taille du modèle, et on va éventuellement utiliser un learning rate scheduler.\n",
    "\n",
    "Pour installer pytorch lightning: `pip install pytorch-lightning`.\n",
    "Si vous êtes sur colab: `!pip install pytorch-lightning`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Preprocessing les données et créer le dataloader\n",
    "\n",
    "Il faut maintenant tokeniser les données et les transformer en liste d'entiers, en utilisant la tokenization BPE qu'on vient d'entraîner.\n",
    "\n",
    "Comme la tokenisation BPE résulte en des listes de tokens de tailles assez longues, il faut les tronquer sinon on va avoir des problèmes de mémoire.\n",
    "On va donc définir `max_len = 1500` et tronquer les listes de tokens à cette taille.\n",
    "Remarque: vous pouvez réduire `max_len` si vous voulez augmenter la taille du modèle ou la batch size.\n",
    "\n",
    "\n",
    "Il faut aussi padder les données pour qu'elles aient la même taille.\n",
    "Dans les TP précédents, vous avez padder à la main, ce qui est assez long. Vous pouvez utiliser `torch.nn.utils.rnn.pad_sequence` pour padder les données.\n",
    "La documentation est ici: https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
    "\n",
    "Il faut aussi créer un dataloader avec `torch.utils.data.DataLoader`, en utilisant `torch.utils.data.TensorDataset` pour créer le dataset qui contient les données tokenisées et paddées, et les masks éventuels.\n",
    "Exemple de dataloader:\n",
    "```python\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, key_padding_mask, loss_mask)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=3)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "...\n",
    "train_loader = ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Création du modèle\n",
    "\n",
    "Implémenter votre transformer-decoder-only ici."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Création du modèle lightning\n",
    "Ici, on va créer le modèle pytorch lightning. Il faut donc créer la fonction `forward` et la fonction `training_step`.\n",
    "Remarque: vous pouvez commencer par un modèle de taille: `d_model = 256`, `n_heads = 8`, `n_layers = 5`.\n",
    "Normalement vous devriez pas avoir de problème de mémoire avec `max_len = 1500` et `batch_size = 16`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class MyModel(pl.LightningModule):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = ...\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\t...\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\t...\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "...\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=1000, precision='16-mixed', callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vous pouvez ajouter ce code dans la fonction `training_step` afin de voir l'output de votre modèle pendant l'entraînement.\n",
    "```python\n",
    "if batch_idx == 0:\n",
    "\tself.model.eval()\n",
    "\tcur_sentence = [bos_id, sp.encode_as_ids(\"I\")[0]]\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor i in range(max_len):\n",
    "\t\t\tcur_input = torch.tensor(cur_sentence).unsqueeze(0).to(self.device)\n",
    "\t\t\toutput = model(cur_input)\n",
    "\t\t\toutput = output[:, -1, :]\n",
    "\t\t\toutput = output[0].argmax()\n",
    "\t\t\tcur_sentence.append(output.item())\n",
    "\t\t\tif output.item() == eos_id:\n",
    "\t\t\t\tbreak\n",
    "\tprint(sp.decode_ids(cur_sentence))\n",
    "\n",
    "\tself.model.train()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Génération de texte\n",
    "Il faut maintenant générer du texte avec notre modèle, en utilisant le sampling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après 30 minutes d'entraînement sur colab, j'ai obtenu:\n",
    "\n",
    "\"I'm so hungry i can't tell people i have the job. Most of the couple things you couldn't pay more than a damn job. I can have a job of money. So i also try to deal with keeping him to make some work. He's a good deal. You know him. He doesn't eaten things sometimes. He thinks he doesn't go crazy is either crazy i'd be a bad shitty writer. You sure as hell knows, but he does n't buyback. I'm gon na make it out to my friend when i get him a lot more excuse for me. I don't care if he can, but i don't really help. I need him to school. He's too good for me. I don't need to be able to go home. Don't care to leave. Don't want to take a drink, i can't leave. Don't look at me like i need him too much to hide in the day. Don't want to go away. Don't drink. Don't drink. Don't drink. Don't listen and take a penis to the cash and then when all i can't even get it. Don't take me on my own phone. Don't we leave to webbing together and started talking to ourselves. Don't you believe me ? maybe i don't want that my friend. I don't want to say something. I don't expect you. I always thought that man is just like me. I don't want to scream when heates. I always wanted to be much better than friendly. I always wanted to be the guy. But not me. I'm never worried about what the fuck people say. I always wanted to talk to you. I always thought about it was just the worst. You know that i'm crazy because i'm pretty sure i'm not saying that because i'll never worried about you grow up and i have a friendship on me. I can't stay there. I've been trying to get away with people. I never hurt anyone but i've been with for so long after i was doomed to ever have the best idea of my life and i'll ever be happy about this guy. I've missed his name, i've got to have the money. I don't care for the way that matter. I always say goodbye. I'll never good enough for the way things i'll ever get to know you. I don't want to know why you grow up with someone else to do with you with your friend.\"\n",
    "\n",
    "### Amélioration possible\n",
    "Vous pouvez essayer d'augmenter la taille de votre modèle, par exemple augmenter `d_model` ou `n_layers`, mais il faudra réduire la batch_size ou `max_len` sinon vous allez avoir des problèmes de mémoire.\n",
    "\n",
    "Vous pouvez aussi implémenter un learning rate scheduler, qui par exemple réduit le learning rate au fur et à mesure.\n",
    "\n",
    "### Warm up learning rate\n",
    "Un learning rate scheduler assez commun pour les transformers c'est le warm up learning rate. Il consiste à augmenter linéairement le learning rate au début de l'entraînement, puis à le réduire au fur et à mesure:\n",
    "\n",
    "![transformer learning rate](https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/learningrate.png)\n",
    "\n",
    "Des recherches ont montré que le warm up learning rate permet d'améliorer la performance des transformers.\n",
    "\n",
    "Il n'y a pas ce scheduler déjà implémenté dans pytorch, donc vous allez devoir l'implémenter vous-même ce custom learning rate scheduler.\n",
    "\n",
    "Pour implémenter un tel custom learning rate scheduler, il faut créer une classe qui hérite de `torch.optim.lr_scheduler.LambdaLR` et implémenter la fonction `lr_lamba`.\n",
    "La fonction `lr_lambda` prend en argument l'epoch ou step et retourne le facteur multiplicatif du learning rate.\n",
    "\n",
    "Par exemple, ici j'ai implémenté un learning rate scheduler qui augmente linéairement le learning rate pendant 1000 steps, puis reste constant:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class CustomSchedule(torch.optim.lr_scheduler.LambdaLR):\n",
    "\tdef __init__(self, optimizer, learning_rate, warm_up_steps, last_epoch=-1):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.warm_up_steps = warm_up_steps\n",
    "\t\tsuper().__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "\tdef lr_lambda(self, step):\n",
    "\t\tif step < self.warm_up_steps:\n",
    "\t\t\treturn float(step) / float(max(1, self.warm_up_steps))\n",
    "\t\treturn 1.0\n",
    "```\n",
    "\n",
    "Puis dans la fonction `configure_optimizers` de votre modèle lightning, vous pouvez utiliser ce scheduler:\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    scheduler = CustomSchedule(optim, 0.001, 1000)\n",
    "    scheduler = {\n",
    "        'scheduler': scheduler,\n",
    "        'interval': 'step',\n",
    "        'frequency': 1\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'optimizer': optim,\n",
    "        'lr_scheduler': scheduler\n",
    "    }\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompts\n",
    "Maintenant, on va ajouter des prompts à notre modèle. On va donc entraîner notre modèle à écrire des histoires à partir d'un prompt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Automatants/projets-de-permanence/master/image-hosting/transfo/colab_train_prompts.txt\"\n",
    "r = requests.get(url)\n",
    "dataset_prompt = r.text\n",
    "\n",
    "# save file\n",
    "with open(\"colab_train_prompts.txt\", \"w\") as f:\n",
    "\tf.write(dataset_prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`dataset_prompt` est une string qui contient les prompts et les histoires associées. Chaque ligne correspond à au prompt associé à chaque histoire de `dataset`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset_prompt.split(\"\\n\")[0])  # On affiche la première histoire\n",
    "print(dataset.split(\"\\n\")[0])  # On affiche la première histoire"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Preprocessing les données et créer le dataloader\n",
    "\n",
    "On va mettre les données sous format: \"{prompt} <sep> {histoire}\", où \"\\<sep>\" est un token qui correspond à une séparation entre le prompt et l'histoire.\n",
    "Exemple:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_prompt.split(\"\\n\")[0] + \" <sep> \" + dataset.split(\"\\n\")[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ajouter le token \\<sep> à la tokenization BPE de sentencepiece\n",
    "\n",
    "Il faut ajouter le token \"\\<sep>\" à la tokenization BPE de sentencepiece pour pas que sentencepiece le découpe en sous-mots.\n",
    "Pour ça, il faut ajouter `--user_defined_symbols=<sep>` dans `sentencepiece_params` et réentraîner la tokenization BPE.\n",
    "```python\n",
    "sentencepiece_params = ' '.join([\n",
    "\t'--input={}'.format(input_file),\n",
    "\t'--model_type={}'.format(model_type),\n",
    "\t'--model_prefix={}'.format(model_prefix),\n",
    "\t'--vocab_size={}'.format(max_num_words),\n",
    "\t'--pad_id={}'.format(pad_id),\n",
    "\t'--unk_id={}'.format(unk_id),\n",
    "\t'--bos_id={}'.format(start_id),\n",
    "\t'--eos_id={}'.format(end_id),\n",
    "\t'--user_defined_symbols=<sep>'\n",
    "])\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Créer un mask pour le prompt\n",
    "Ça ne sert à rien d'entrainer le modèle à prédire le prompt, donc il faut créer un mask pour le prompt, et le multiplier par la loss pour que le modèle ne prédise pas le prompt.\n",
    "Par exemple: \"Le secret de l'immortalité \\<sep> histoire ...\" alors le mask va être [0, 0, 0, 0, 0, 1, 1, ...] où 0 correspond à chaque token du prompt et 1 correspond à l'histoire."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice: Création du modèle lightning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
